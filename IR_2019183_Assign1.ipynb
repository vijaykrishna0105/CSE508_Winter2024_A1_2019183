{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt-VPMATJwV8",
        "outputId": "e870ea11-74d3-40d9-ab29-22ae128da0eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cgu6Ghb30jaU",
        "outputId": "afd73cc1-7752-462d-a0bc-4960f0cd91f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "epF-wpQ36oly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1. Data Preprocessing**\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "NPdri1Qa6o-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# Path to the dataset directory\n",
        "dataset_dir = '/content/drive/MyDrive/IR_DataSet_Files/text_files-20240206T164431Z-001/text_files'\n",
        "\n",
        "\n",
        "all_files = os.listdir(dataset_dir)\n",
        "\n",
        "\n",
        "random_files = random.sample(all_files, 5)\n"
      ],
      "metadata": {
        "id": "kbHY7KLf6uW8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb9b5ef-8208-4e13-af97-013a942f08fe"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in random_files:\n",
        "    file_path = os.path.join(dataset_dir, filename)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "        # Printing  original content\n",
        "        print(f\"Original content of {filename}:\\n{content}\\n\")\n",
        "\n",
        "        # Lowercase the text of the original content\n",
        "        lowercased_content = content.lower()\n",
        "\n",
        "        # Print lowercased content after the lowering as a step1.\n",
        "        print(f\"Lowercased content of {filename}:\\n{lowercased_content}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o55k3TCo7DGR",
        "outputId": "68eeee0f-4895-473a-b07d-061f204c8d64"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original content of file516.txt:\n",
            "I'm using several of these on a wall I built. The hangers work great with several different head-stock shapes. I wish there were available bands, straps or locking devices to keep instruments secure here in earthquake prone California. I ended up using velcro strips under the hangers that affix over the fretboard. Otherwise a great hanger..\n",
            "\n",
            "Lowercased content of file516.txt:\n",
            "i'm using several of these on a wall i built. the hangers work great with several different head-stock shapes. i wish there were available bands, straps or locking devices to keep instruments secure here in earthquake prone california. i ended up using velcro strips under the hangers that affix over the fretboard. otherwise a great hanger..\n",
            "\n",
            "Original content of file255.txt:\n",
            "Mine were very stiff and hard to tune, no smoothness, like there was dirt or grit in the gears.  After installing one of the tuning keys got super stiff and before I could get up to tune the head snapped off.  Had to return entire set...pain in the azz.  Not worth the money in savings, going with Hipshots next.\n",
            "\n",
            "Lowercased content of file255.txt:\n",
            "mine were very stiff and hard to tune, no smoothness, like there was dirt or grit in the gears.  after installing one of the tuning keys got super stiff and before i could get up to tune the head snapped off.  had to return entire set...pain in the azz.  not worth the money in savings, going with hipshots next.\n",
            "\n",
            "Original content of file303.txt:\n",
            "We used this one in the middle as seen in the picture holding the white guitar. BIG MISTAKE! The guitar almost was destroyed. This is NOT strong enough to hold a 7 string Schecter Omen guitar. While it worked out and seemed secure at first. Literally 7 days later the guitar had almost fell to the floor. I wouldn't recommend this for heavier guitars. Instead we will be keeping the guitar in the stand where it is safer.\n",
            "\n",
            "Lowercased content of file303.txt:\n",
            "we used this one in the middle as seen in the picture holding the white guitar. big mistake! the guitar almost was destroyed. this is not strong enough to hold a 7 string schecter omen guitar. while it worked out and seemed secure at first. literally 7 days later the guitar had almost fell to the floor. i wouldn't recommend this for heavier guitars. instead we will be keeping the guitar in the stand where it is safer.\n",
            "\n",
            "Original content of file215.txt:\n",
            "It's great for my Taylor 150e 12-string guitar.... My only issue is that the wand it came with had a tear in the microfiber duster.\n",
            "\n",
            "Lowercased content of file215.txt:\n",
            "it's great for my taylor 150e 12-string guitar.... my only issue is that the wand it came with had a tear in the microfiber duster.\n",
            "\n",
            "Original content of file658.txt:\n",
            "Phat,PHILTHY AND JUST PLAIN ON ALL THE TIME!,Grinderthru the THUNDERVERB!\n",
            "\n",
            "Lowercased content of file658.txt:\n",
            "phat,philthy and just plain on all the time!,grinderthru the thunderverb!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perform tokenization**"
      ],
      "metadata": {
        "id": "2Ypsvdt07o03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Iterate over each of the 5 randomly selected files\n",
        "for filename in random_files:\n",
        "    file_path = os.path.join(dataset_dir, filename)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "        # Print original content\n",
        "        print(f\"Original content of {filename}:\\n{content}\\n\")\n",
        "\n",
        "        # Lowercase the text\n",
        "        lowercased_content = content.lower()\n",
        "\n",
        "        # Tokenize the lowercased content\n",
        "        tokens = word_tokenize(lowercased_content)\n",
        "\n",
        "        # Print tokenized content after the process.\n",
        "        print(f\"Tokenized content of {filename}:\\n{tokens}\\n\")\n",
        "\n",
        "        # Removing  stopwords\n",
        "        tokens_without_stopwords = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "        # Print the content after removing stopwords\n",
        "        print(f\"Content of {filename} after removing stopwords:\\n{' '.join(tokens_without_stopwords)}\\n\")\n",
        "\n",
        "\n",
        "        # Remove punctuations and blank space tokens\n",
        "        tokens_without_punctuations_and_spaces = [word for word in tokens_without_stopwords if word not in string.punctuation and word.strip()]\n",
        "\n",
        "        # Print the content after removing punctuations and blank spaces\n",
        "        print(f\"Content of {filename} after removing punctuations and blank spaces:\\n{' '.join(tokens_without_punctuations_and_spaces)}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szflZF2G7yRZ",
        "outputId": "970cf000-f46c-4266-88b3-e67492a4b25f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original content of file516.txt:\n",
            "I'm using several of these on a wall I built. The hangers work great with several different head-stock shapes. I wish there were available bands, straps or locking devices to keep instruments secure here in earthquake prone California. I ended up using velcro strips under the hangers that affix over the fretboard. Otherwise a great hanger..\n",
            "\n",
            "Tokenized content of file516.txt:\n",
            "['i', \"'m\", 'using', 'several', 'of', 'these', 'on', 'a', 'wall', 'i', 'built', '.', 'the', 'hangers', 'work', 'great', 'with', 'several', 'different', 'head-stock', 'shapes', '.', 'i', 'wish', 'there', 'were', 'available', 'bands', ',', 'straps', 'or', 'locking', 'devices', 'to', 'keep', 'instruments', 'secure', 'here', 'in', 'earthquake', 'prone', 'california', '.', 'i', 'ended', 'up', 'using', 'velcro', 'strips', 'under', 'the', 'hangers', 'that', 'affix', 'over', 'the', 'fretboard', '.', 'otherwise', 'a', 'great', 'hanger', '..']\n",
            "\n",
            "Content of file516.txt after removing stopwords:\n",
            "'m using several wall built . hangers work great several different head-stock shapes . wish available bands , straps locking devices keep instruments secure earthquake prone california . ended using velcro strips hangers affix fretboard . otherwise great hanger ..\n",
            "\n",
            "Content of file516.txt after removing punctuations and blank spaces:\n",
            "'m using several wall built hangers work great several different head-stock shapes wish available bands straps locking devices keep instruments secure earthquake prone california ended using velcro strips hangers affix fretboard otherwise great hanger ..\n",
            "\n",
            "Original content of file255.txt:\n",
            "Mine were very stiff and hard to tune, no smoothness, like there was dirt or grit in the gears.  After installing one of the tuning keys got super stiff and before I could get up to tune the head snapped off.  Had to return entire set...pain in the azz.  Not worth the money in savings, going with Hipshots next.\n",
            "\n",
            "Tokenized content of file255.txt:\n",
            "['mine', 'were', 'very', 'stiff', 'and', 'hard', 'to', 'tune', ',', 'no', 'smoothness', ',', 'like', 'there', 'was', 'dirt', 'or', 'grit', 'in', 'the', 'gears', '.', 'after', 'installing', 'one', 'of', 'the', 'tuning', 'keys', 'got', 'super', 'stiff', 'and', 'before', 'i', 'could', 'get', 'up', 'to', 'tune', 'the', 'head', 'snapped', 'off', '.', 'had', 'to', 'return', 'entire', 'set', '...', 'pain', 'in', 'the', 'azz', '.', 'not', 'worth', 'the', 'money', 'in', 'savings', ',', 'going', 'with', 'hipshots', 'next', '.']\n",
            "\n",
            "Content of file255.txt after removing stopwords:\n",
            "mine stiff hard tune , smoothness , like dirt grit gears . installing one tuning keys got super stiff could get tune head snapped . return entire set ... pain azz . worth money savings , going hipshots next .\n",
            "\n",
            "Content of file255.txt after removing punctuations and blank spaces:\n",
            "mine stiff hard tune smoothness like dirt grit gears installing one tuning keys got super stiff could get tune head snapped return entire set ... pain azz worth money savings going hipshots next\n",
            "\n",
            "Original content of file303.txt:\n",
            "We used this one in the middle as seen in the picture holding the white guitar. BIG MISTAKE! The guitar almost was destroyed. This is NOT strong enough to hold a 7 string Schecter Omen guitar. While it worked out and seemed secure at first. Literally 7 days later the guitar had almost fell to the floor. I wouldn't recommend this for heavier guitars. Instead we will be keeping the guitar in the stand where it is safer.\n",
            "\n",
            "Tokenized content of file303.txt:\n",
            "['we', 'used', 'this', 'one', 'in', 'the', 'middle', 'as', 'seen', 'in', 'the', 'picture', 'holding', 'the', 'white', 'guitar', '.', 'big', 'mistake', '!', 'the', 'guitar', 'almost', 'was', 'destroyed', '.', 'this', 'is', 'not', 'strong', 'enough', 'to', 'hold', 'a', '7', 'string', 'schecter', 'omen', 'guitar', '.', 'while', 'it', 'worked', 'out', 'and', 'seemed', 'secure', 'at', 'first', '.', 'literally', '7', 'days', 'later', 'the', 'guitar', 'had', 'almost', 'fell', 'to', 'the', 'floor', '.', 'i', 'would', \"n't\", 'recommend', 'this', 'for', 'heavier', 'guitars', '.', 'instead', 'we', 'will', 'be', 'keeping', 'the', 'guitar', 'in', 'the', 'stand', 'where', 'it', 'is', 'safer', '.']\n",
            "\n",
            "Content of file303.txt after removing stopwords:\n",
            "used one middle seen picture holding white guitar . big mistake ! guitar almost destroyed . strong enough hold 7 string schecter omen guitar . worked seemed secure first . literally 7 days later guitar almost fell floor . would n't recommend heavier guitars . instead keeping guitar stand safer .\n",
            "\n",
            "Content of file303.txt after removing punctuations and blank spaces:\n",
            "used one middle seen picture holding white guitar big mistake guitar almost destroyed strong enough hold 7 string schecter omen guitar worked seemed secure first literally 7 days later guitar almost fell floor would n't recommend heavier guitars instead keeping guitar stand safer\n",
            "\n",
            "Original content of file215.txt:\n",
            "It's great for my Taylor 150e 12-string guitar.... My only issue is that the wand it came with had a tear in the microfiber duster.\n",
            "\n",
            "Tokenized content of file215.txt:\n",
            "['it', \"'s\", 'great', 'for', 'my', 'taylor', '150e', '12-string', 'guitar', '....', 'my', 'only', 'issue', 'is', 'that', 'the', 'wand', 'it', 'came', 'with', 'had', 'a', 'tear', 'in', 'the', 'microfiber', 'duster', '.']\n",
            "\n",
            "Content of file215.txt after removing stopwords:\n",
            "'s great taylor 150e 12-string guitar .... issue wand came tear microfiber duster .\n",
            "\n",
            "Content of file215.txt after removing punctuations and blank spaces:\n",
            "'s great taylor 150e 12-string guitar .... issue wand came tear microfiber duster\n",
            "\n",
            "Original content of file658.txt:\n",
            "Phat,PHILTHY AND JUST PLAIN ON ALL THE TIME!,Grinderthru the THUNDERVERB!\n",
            "\n",
            "Tokenized content of file658.txt:\n",
            "['phat', ',', 'philthy', 'and', 'just', 'plain', 'on', 'all', 'the', 'time', '!', ',', 'grinderthru', 'the', 'thunderverb', '!']\n",
            "\n",
            "Content of file658.txt after removing stopwords:\n",
            "phat , philthy plain time ! , grinderthru thunderverb !\n",
            "\n",
            "Content of file658.txt after removing punctuations and blank spaces:\n",
            "phat philthy plain time grinderthru thunderverb\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define your source and target directories\n",
        "source_dir = '/content/drive/MyDrive/IR_DataSet_Files/text_files-20240206T164431Z-001/text_files'\n",
        "target_dir = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessed_Files'\n",
        "\n",
        "# Create the target directory if it doesn't exist\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n"
      ],
      "metadata": {
        "id": "SyJfiPsdfHbI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Remove punctuations and blank space tokens\n",
        "    tokens = [word for word in tokens if word not in string.punctuation and word.strip()]\n",
        "    # Rejoin tokens into a string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "    return preprocessed_text\n"
      ],
      "metadata": {
        "id": "Jx9SvBetiPzI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove all punctuation from the text, including apostrophes, but preserve intra-word apostrophes\n",
        "    text = re.sub(r'\\b\\'|\\'\\b', '', text)  # Remove leading and trailing apostrophes\n",
        "    text = re.sub(r'(?<!\\w)[^\\s\\w]+|[^\\s\\w]+(?!\\w)', '', text)  # Remove punctuation without removing intra-word apostrophes\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "# Example usage\n",
        "original_text = \"it's more on the toy side than on the instrument side, and made in Indonesia.\"\n",
        "preprocessed_tokens = preprocess_text(original_text)\n",
        "print(\"Preprocessed text:\", ' '.join(preprocessed_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2bsvFap_wXk",
        "outputId": "439f8a27-ed9b-46d0-f5b0-94a692d1ee11"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed text: toy side instrument side made indonesia\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for filename in os.listdir(source_dir):\n",
        "#     file_path = os.path.join(source_dir, filename)\n",
        "#     # Ensure we're only working with files (and, for example, not directories)\n",
        "#     if os.path.isfile(file_path):\n",
        "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
        "#             content = file.read()\n",
        "#             preprocessed_content = preprocess_text(content)\n",
        "#             # Define the path for the new file in the target directory\n",
        "#             new_file_path = os.path.join(target_dir, filename)\n",
        "#             with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
        "#                 new_file.write(preprocessed_content)\n"
      ],
      "metadata": {
        "id": "djTmXRrLiVht"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define your source and target directories\n",
        "source_dir = '/content/drive/MyDrive/IR_DataSet_Files/text_files-20240206T164431Z-001/text_files'\n",
        "target_dir = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessed_Files'\n",
        "\n",
        "# Get a list of filenames in the source directory (ensure consistent ordering)\n",
        "source_files = sorted(os.listdir(source_dir))[:5]  # Only take the first 5 files for demonstration\n",
        "\n",
        "for filename in source_files:\n",
        "    # Define the paths for the source and target files\n",
        "    source_file_path = os.path.join(source_dir, filename)\n",
        "    target_file_path = os.path.join(target_dir, filename)\n",
        "\n",
        "    # Print original content\n",
        "    with open(source_file_path, 'r', encoding='utf-8') as file:\n",
        "        original_content = file.read()\n",
        "        print(f\"Original content of {filename}:\\n{original_content}\\n\")\n",
        "\n",
        "    # Ensure the preprocessed file exists before attempting to open it\n",
        "    if os.path.exists(target_file_path):\n",
        "        # Print preprocessed content\n",
        "        with open(target_file_path, 'r', encoding='utf-8') as file:\n",
        "            preprocessed_content = file.read()\n",
        "            print(f\"Preprocessed content of {filename}:\\n{preprocessed_content}\\n\")\n",
        "    else:\n",
        "        print(f\"No preprocessed content found for {filename}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF7aGTfnjaUZ",
        "outputId": "bbdba248-7c32-4755-deb8-909d5e363c72"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original content of file1.txt:\n",
            "Loving these vintage springs on my vintage strat. They have a good tension and great stability. If you are floating your bridge and want the most out of your springs than these are the way to go.\n",
            "\n",
            "Preprocessed content of file1.txt:\n",
            "loving vintage springs vintage strat good tension great stability floating bridge want springs way go\n",
            "\n",
            "Original content of file10.txt:\n",
            "Awesome stand!\n",
            "\n",
            "Tip: The bottom part that supports the guitar had a weird angle when arrived, making the guitar slide back, becoming almost 100% on a vertical.\n",
            "To solve this, I assembled the product and the put a some pressure on the support frame, making it bend a little. Now my guitar sits perfectly. Check photos!\n",
            "\n",
            "Preprocessed content of file10.txt:\n",
            "awesome stand tip bottom part supports guitar weird angle arrived making guitar slide back becoming almost 100 vertical solve assembled product put pressure support frame making bend little guitar sits perfectly check photos\n",
            "\n",
            "Original content of file100.txt:\n",
            "This amp is the real deal.  Great crunch and gain tones and with some tweaking, not half bad clean\"ish\" tones.  I've played this through the two 8\" Orange cabs (had to get those too as they were just TOO cool ((and cute)) and not crazy money) and the sound is very pleasing and revealing for a practice amp.  I primarily play it through my Blackstar stack that I've fitted with Celestion V30s... Wow...there it is~!!!  You would never know this thing was such a tone monster... Even with just a few knobs it's easy to get lost for hours playing this thing.  My favorite match is with my Chapman ML-1 Hotrod...which only has a volume \"tone\" control (EVH fans get this).  Not a lot of mucking around with too many knobs or too many options on either the guitar or this amp... Just tone up and go.  I see the Micro Dark just came out...that's probably next~!  Higher gain, buffered effects loop and speaker emu at the headphone out for recording direct (if that's your thing).\n",
            "\n",
            "Preprocessed content of file100.txt:\n",
            "amp real deal great crunch gain tones tweaking half bad clean '' ish '' tones 've played two 8 '' orange cabs get cool cute crazy money sound pleasing revealing practice amp primarily play blackstar stack 've fitted celestion v30s ... wow ... is~ would never know thing tone monster ... even knobs 's easy get lost hours playing thing favorite match chapman ml-1 hotrod ... volume `` tone '' control evh fans get lot mucking around many knobs many options either guitar amp ... tone go see micro dark came ... 's probably next~ higher gain buffered effects loop speaker emu headphone recording direct 's thing\n",
            "\n",
            "Original content of file101.txt:\n",
            "You can do a lot with this mixer. its great for podcasting. has 4 outputs that can be used to monitor, record, cue audio...The mute to 3/4 figure on every channel is fantastic and the three source switch to headphone/control room is a must for podcasting. Also has aux return inputs that can be used as extra stereo inputs and be volumed by the aux return knobs.\n",
            "\n",
            "Only thing I didn't like about this mixer is the XLR outputs in back that require adaptors to use with RCA or 1/4 plugs. get the adaptors with it\n",
            "\n",
            "Preprocessed content of file101.txt:\n",
            "lot mixer great podcasting 4 outputs used monitor record cue audio ... mute 3/4 figure every channel fantastic three source switch headphone/control room must podcasting also aux return inputs used extra stereo inputs volumed aux return knobs thing n't like mixer xlr outputs back require adaptors use rca 1/4 plugs get adaptors\n",
            "\n",
            "Original content of file102.txt:\n",
            "<div id=\"video-block-R2VOQ5CBZHFCKL\" class=\"a-section a-spacing-small a-spacing-top-mini video-block\"></div><input type=\"hidden\" name=\"\" value=\"https://images-na.ssl-images-amazon.com/images/I/E1%2B6MhK2MfS.mp4\" class=\"video-url\"><input type=\"hidden\" name=\"\" value=\"https://images-na.ssl-images-amazon.com/images/I/21-Jk5lxqsS.png\" class=\"video-slate-img-url\">&nbsp;This mic is a BOSS and a lot better than just about any other mic I've seen or used out-of-the-box for voice over. It sounds great even before processing, and with some compression and EQ, it sounds fantastic. It rejects a ton of background noise and sounds amazing.\n",
            "\n",
            "It runs very HOT! So you'll want clean pre-amping as to get a clean signal, but this is an amazing mic for the price.\n",
            "\n",
            "Preprocessed content of file102.txt:\n",
            "div id= '' video-block-r2voq5cbzhfckl '' class= '' a-section a-spacing-small a-spacing-top-mini video-block '' /div input type= '' hidden '' name= '' '' value= '' https //images-na.ssl-images-amazon.com/images/i/e1 2b6mhk2mfs.mp4 '' class= '' video-url '' input type= '' hidden '' name= '' '' value= '' https //images-na.ssl-images-amazon.com/images/i/21-jk5lxqss.png '' class= '' video-slate-img-url '' nbsp mic boss lot better mic 've seen used out-of-the-box voice sounds great even processing compression eq sounds fantastic rejects ton background noise sounds amazing runs hot 'll want clean pre-amping get clean signal amazing mic price\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2. Unigram Inverted Index and Boolean Queries**"
      ],
      "metadata": {
        "id": "SLMJX_lQBZfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SNIbYLIi9OVh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "En3Y2JM378D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Define the path to your consolidated preprocessed text file\n",
        "output_file_path = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/consolidated_preprocessed.txt'\n",
        "\n",
        "# Initialize an empty dictionary for the inverted index\n",
        "inverted_index = {}\n",
        "\n",
        "# Open and read the preprocessed file\n",
        "with open(output_file_path, 'r', encoding='utf-8') as file:\n",
        "    # Assume each line corresponds to a document\n",
        "    for doc_id, line in enumerate(file):\n",
        "        # Tokenize the line into unigrams (since it's already preprocessed)\n",
        "        unigrams = line.strip().split()\n",
        "        # Update the inverted index\n",
        "        for unigram in unigrams:\n",
        "            if unigram in inverted_index:\n",
        "                inverted_index[unigram].add(doc_id)\n",
        "            else:\n",
        "                inverted_index[unigram] = {doc_id}\n",
        "\n",
        "#Serializing the Inverted Index with Pickle\n",
        "\n",
        "# Define a path to save the serialized inverted index\n",
        "index_file_path = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/inverted_index.pkl'\n",
        "\n",
        "# Serialize and save the inverted index using pickle\n",
        "with open(index_file_path, 'wb') as index_file:\n",
        "    pickle.dump(inverted_index, index_file)\n",
        "\n",
        "print(f\"Inverted index created and saved to {index_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAWAeroJBgjV",
        "outputId": "c001a56f-9429-46e8-8aaf-e1da163818b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inverted index created and saved to /content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/inverted_index.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Path to your serialized inverted index\n",
        "index_file_path = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/inverted_index.pkl'\n",
        "\n",
        "# Load the inverted index from the file\n",
        "with open(index_file_path, 'rb') as index_file:\n",
        "    inverted_index = pickle.load(index_file)\n",
        "\n",
        "\n",
        "sample_terms = list(inverted_index.keys())[:10]  # Adjust the slice as needed\n",
        "\n",
        "for term in sample_terms:\n",
        "    doc_ids = inverted_index[term]\n",
        "    # Filter doc_ids to only include those from the first 10 documents\n",
        "    filtered_doc_ids = {doc_id for doc_id in doc_ids if doc_id < 10}\n",
        "    if filtered_doc_ids:\n",
        "        print(f\"Term '{term}' appears in documents: {sorted(filtered_doc_ids)}\")\n",
        "    else:\n",
        "        print(f\"Term '{term}' does not appear in the first 10 documents.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmgbD9EGBuqQ",
        "outputId": "75f6af16-987c-4ec7-9527-539d68a6e8e3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term 'took' appears in documents: [0, 7]\n",
            "Term 'time' appears in documents: [0, 7]\n",
            "Term 'get' appears in documents: [0, 3, 4]\n",
            "Term 'set' appears in documents: [0, 3]\n",
            "Term 'right' appears in documents: [0]\n",
            "Term 'really' appears in documents: [0, 4, 7]\n",
            "Term 'like' appears in documents: [0, 2, 3]\n",
            "Term 'awesome' appears in documents: [1]\n",
            "Term 'thumb' appears in documents: [1]\n",
            "Term 'pic' appears in documents: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Specify the path to your serialized inverted index .pkl file\n",
        "index_file_path = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/inverted_index.pkl'\n",
        "\n",
        "# Use 'rb' to read in binary mode\n",
        "with open(index_file_path, 'rb') as index_file:\n",
        "    loaded_inverted_index = pickle.load(index_file)\n",
        "\n",
        "print(\"Inverted index loaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Akzeah-vln_",
        "outputId": "3b6efa9c-8b12-486d-cd0f-69f8047e3492"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inverted index loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 10 keys (terms) in the inverted index\n",
        "for term in list(inverted_index.keys())[:10]:\n",
        "    print(term)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OqrJ_iB2vtO",
        "outputId": "e0bad5c0-8ce2-468d-b574-f67826a280f9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "took\n",
            "time\n",
            "get\n",
            "set\n",
            "right\n",
            "really\n",
            "like\n",
            "awesome\n",
            "thumb\n",
            "pic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Replace 'sample_term' with the actual term you're interested in\n",
        "# sample_terms = ['sample_term1', 'sample_term2']\n",
        "\n",
        "# for term in sample_terms:\n",
        "#     if term in inverted_index:\n",
        "#         print(f\"Documents containing '{term}': {inverted_index[term]}\")\n",
        "#     else:\n",
        "#         print(f\"'{term}' not found in the index.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ApxfrCx8I41",
        "outputId": "2fa52b42-3ba3-41ec-bcc7-091dd5a51764"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'sample_term1' not found in the index.\n",
            "'sample_term2' not found in the index.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the document count for the first 10 terms\n",
        "#Checking the sample terms where it appears in docs\n",
        "for term in list(inverted_index.keys())[:10]:\n",
        "    doc_count = len(inverted_index[term])\n",
        "    print(f\"'{term}' appears in {doc_count} documents\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_Dip7Yz9USh",
        "outputId": "b4f083cf-b83b-4a41-8e19-aeae411ad441"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'took' appears in 38 documents\n",
            "'time' appears in 92 documents\n",
            "'get' appears in 172 documents\n",
            "'set' appears in 81 documents\n",
            "'right' appears in 80 documents\n",
            "'really' appears in 137 documents\n",
            "'like' appears in 209 documents\n",
            "'awesome' appears in 49 documents\n",
            "'thumb' appears in 6 documents\n",
            "'pic' appears in 12 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# target directory containing preprocessed files\n",
        "target_dir = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessed_Files'\n",
        "\n",
        "#  the universe set from filenames in the target directory\n",
        "universe_set = {filename for filename in os.listdir(target_dir)}\n",
        "\n",
        "print(\"Universe set defined successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv3vLsUdGETP",
        "outputId": "bfd47b14-9390-4840-d932-f6ae9dce3aa5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Universe set defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query_and(set1, set2):\n",
        "    return set1 & set2\n",
        "\n",
        "def query_or(set1, set2):\n",
        "    return set1 | set2\n",
        "\n",
        "def query_and_not(set1, set2):\n",
        "    return set1 - set2\n",
        "\n",
        "\n",
        "def query_or_not(set1, set2, universe_set):\n",
        "    return set1 | (universe_set - set2)\n"
      ],
      "metadata": {
        "id": "q0NkAyxyGBkk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "index_file_path = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/inverted_index.pkl'\n",
        "\n",
        "\n",
        "with open(index_file_path, 'rb') as index_file:\n",
        "    inverted_index = pickle.load(index_file)\n",
        "\n",
        "# Printing the first 50 entries of the inverted index for checking how looks\n",
        "for i, (term, doc_ids) in enumerate(inverted_index.items()):\n",
        "    print(f\"Term '{term}' appears in documents: {doc_ids}\")\n",
        "    if i >= 49:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIuTr7kDLXnL",
        "outputId": "6e171b62-40dd-46c0-d3f4-9ff3a6f5b707"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term 'took' appears in documents: {0, 515, 7, 783, 144, 658, 919, 408, 411, 925, 416, 545, 929, 547, 938, 558, 686, 49, 698, 577, 194, 322, 966, 200, 969, 335, 339, 472, 345, 485, 999, 108, 749, 881, 243, 374, 249, 892}\n",
            "Term 'time' appears in documents: {0, 514, 7, 526, 15, 542, 33, 37, 562, 563, 572, 63, 68, 76, 600, 605, 95, 608, 100, 612, 107, 624, 113, 129, 643, 651, 657, 658, 150, 669, 672, 161, 163, 676, 165, 680, 182, 694, 702, 203, 723, 213, 215, 737, 241, 759, 250, 260, 270, 273, 275, 797, 286, 292, 293, 819, 310, 312, 831, 324, 838, 853, 342, 868, 364, 367, 880, 882, 883, 884, 886, 890, 891, 894, 905, 397, 398, 911, 402, 404, 411, 924, 415, 927, 424, 954, 447, 973, 463, 992, 485, 999}\n",
            "Term 'get' appears in documents: {0, 3, 4, 517, 519, 520, 524, 527, 533, 538, 540, 38, 40, 552, 556, 557, 558, 48, 560, 50, 51, 562, 54, 58, 571, 64, 65, 577, 68, 583, 72, 73, 587, 87, 600, 603, 108, 114, 626, 129, 131, 137, 650, 654, 145, 152, 160, 165, 684, 694, 187, 699, 189, 702, 191, 195, 707, 197, 198, 710, 711, 715, 717, 206, 213, 729, 730, 731, 732, 224, 737, 226, 738, 234, 238, 241, 754, 245, 760, 762, 764, 773, 780, 270, 271, 784, 273, 275, 788, 279, 282, 796, 289, 293, 807, 811, 813, 820, 825, 314, 828, 317, 829, 830, 320, 329, 331, 844, 335, 849, 856, 345, 352, 867, 868, 872, 873, 364, 881, 883, 884, 885, 376, 378, 379, 380, 890, 382, 894, 384, 898, 904, 908, 402, 914, 404, 916, 919, 408, 921, 925, 929, 418, 419, 932, 935, 426, 938, 428, 430, 943, 436, 445, 446, 959, 450, 454, 967, 458, 973, 975, 472, 475, 478, 992, 486, 999, 1002, 1003, 497, 498, 499}\n",
            "Term 'set' appears in documents: {0, 3, 520, 522, 11, 19, 28, 548, 562, 52, 567, 70, 608, 109, 120, 124, 126, 139, 658, 147, 155, 163, 167, 168, 682, 691, 182, 695, 195, 197, 202, 714, 213, 749, 751, 241, 755, 763, 767, 804, 814, 303, 305, 817, 821, 313, 827, 317, 837, 838, 336, 354, 871, 873, 874, 884, 887, 381, 382, 398, 914, 403, 407, 411, 927, 416, 418, 930, 933, 935, 943, 971, 460, 977, 483, 995, 999, 1002, 491, 504, 508}\n",
            "Term 'right' appears in documents: {0, 519, 520, 11, 19, 533, 22, 24, 38, 40, 44, 557, 52, 569, 57, 572, 573, 68, 584, 587, 77, 597, 98, 617, 108, 620, 627, 129, 131, 133, 678, 679, 168, 171, 691, 183, 697, 721, 213, 729, 234, 239, 242, 754, 761, 250, 765, 263, 775, 265, 784, 273, 279, 802, 294, 315, 317, 321, 834, 324, 839, 331, 347, 859, 863, 352, 356, 883, 392, 406, 410, 925, 943, 436, 959, 459, 478, 990, 999, 500}\n",
            "Term 'really' appears in documents: {0, 4, 516, 518, 7, 519, 520, 521, 527, 25, 541, 544, 546, 549, 552, 43, 559, 48, 562, 57, 573, 62, 76, 78, 84, 89, 610, 612, 613, 614, 619, 620, 111, 623, 122, 635, 124, 126, 649, 140, 656, 147, 152, 669, 670, 165, 168, 682, 683, 684, 174, 189, 703, 709, 203, 723, 213, 216, 731, 734, 735, 227, 231, 234, 750, 760, 248, 773, 777, 780, 278, 279, 282, 286, 294, 302, 307, 819, 312, 313, 828, 320, 838, 839, 336, 848, 852, 853, 345, 862, 352, 355, 356, 359, 871, 873, 364, 367, 883, 374, 887, 889, 378, 379, 891, 895, 384, 385, 387, 388, 902, 905, 914, 406, 919, 410, 413, 929, 423, 936, 428, 432, 445, 448, 452, 975, 977, 979, 479, 999, 488, 489, 1003, 492, 500, 503, 511}\n",
            "Term 'like' appears in documents: {0, 2, 3, 519, 521, 10, 524, 14, 527, 528, 17, 529, 20, 533, 25, 541, 30, 546, 549, 550, 555, 46, 560, 562, 563, 52, 54, 55, 571, 573, 63, 65, 581, 70, 583, 76, 593, 84, 85, 89, 602, 93, 95, 97, 610, 612, 614, 618, 107, 620, 110, 624, 117, 119, 120, 124, 637, 129, 646, 138, 140, 655, 145, 658, 660, 154, 669, 165, 171, 172, 173, 693, 694, 185, 188, 189, 190, 191, 192, 195, 707, 197, 711, 203, 715, 205, 717, 719, 209, 213, 214, 727, 216, 730, 219, 221, 226, 738, 228, 740, 233, 748, 749, 238, 239, 240, 754, 755, 245, 247, 248, 760, 762, 256, 260, 778, 781, 271, 783, 274, 275, 791, 282, 293, 809, 300, 301, 305, 818, 307, 308, 822, 313, 314, 825, 829, 318, 336, 853, 343, 855, 856, 348, 860, 350, 351, 871, 872, 367, 374, 886, 888, 377, 894, 385, 900, 395, 396, 909, 910, 912, 402, 914, 408, 922, 414, 415, 927, 929, 418, 419, 932, 422, 938, 939, 428, 940, 941, 431, 436, 948, 953, 961, 451, 454, 457, 460, 973, 976, 978, 979, 985, 986, 992, 483, 995, 486, 998, 488, 999, 1001, 1002, 493, 494, 495, 504, 505, 506, 507}\n",
            "Term 'awesome' appears in documents: {1, 516, 137, 395, 16, 529, 913, 19, 278, 662, 412, 284, 413, 542, 544, 801, 675, 165, 425, 810, 813, 180, 436, 821, 952, 57, 833, 582, 328, 76, 461, 463, 977, 340, 598, 119, 89, 350, 606, 352, 612, 1000, 107, 367, 497, 373, 887, 121, 126}\n",
            "Term 'thumb' appears in documents: {1, 97, 294, 1000, 83, 540}\n",
            "Term 'pic' appears in documents: {1, 98, 737, 101, 1000, 682, 204, 883, 886, 729, 635, 316}\n",
            "Term 'use' appears in documents: {2, 517, 8, 17, 530, 22, 23, 28, 542, 543, 31, 551, 554, 555, 45, 46, 47, 48, 562, 563, 52, 56, 57, 568, 570, 575, 64, 65, 576, 68, 69, 580, 71, 583, 585, 74, 75, 586, 587, 83, 596, 603, 97, 107, 619, 622, 623, 118, 124, 637, 639, 127, 642, 643, 645, 646, 140, 659, 150, 155, 157, 669, 670, 163, 165, 677, 678, 679, 685, 689, 691, 693, 182, 694, 184, 185, 701, 712, 201, 202, 715, 209, 211, 214, 215, 219, 222, 734, 735, 736, 226, 738, 233, 235, 240, 752, 755, 248, 250, 766, 256, 775, 264, 778, 788, 796, 797, 286, 799, 803, 299, 813, 305, 310, 311, 827, 829, 323, 324, 835, 838, 327, 328, 839, 841, 842, 845, 335, 851, 341, 343, 345, 858, 868, 358, 872, 878, 881, 883, 377, 889, 891, 385, 899, 388, 391, 392, 903, 904, 910, 912, 402, 916, 917, 406, 918, 411, 413, 926, 419, 422, 934, 936, 427, 428, 947, 437, 442, 959, 960, 449, 454, 968, 974, 976, 982, 989, 478, 482, 996, 486, 998, 1001, 491, 496, 504}\n",
            "Term 'nbsp' appears in documents: {256, 2, 537, 674, 817, 450, 336, 254, 214, 119, 986, 219, 226, 743, 1001, 875, 876, 754, 887, 126}\n",
            "Term 'data-hook=' appears in documents: {674, 226, 2, 743, 1001, 875, 336, 754, 661, 119, 219, 254}\n",
            "Term '''' appears in documents: {2, 3, 4, 7, 537, 30, 40, 552, 44, 557, 46, 49, 564, 568, 571, 583, 73, 81, 97, 100, 101, 613, 105, 619, 620, 119, 126, 129, 646, 654, 145, 658, 661, 674, 163, 165, 682, 694, 704, 707, 711, 715, 214, 727, 729, 219, 222, 734, 226, 738, 228, 743, 238, 239, 240, 752, 754, 246, 254, 256, 260, 773, 263, 778, 269, 271, 272, 788, 789, 284, 294, 298, 817, 308, 310, 823, 317, 832, 835, 336, 849, 345, 350, 360, 873, 875, 876, 883, 885, 887, 384, 907, 915, 406, 918, 919, 410, 922, 414, 416, 933, 934, 423, 936, 427, 428, 431, 953, 442, 450, 453, 463, 977, 466, 471, 472, 474, 986, 992, 482, 492, 496, 504}\n",
            "Term 'product-link-linked' appears in documents: {674, 226, 2, 743, 1001, 875, 336, 754, 661, 119, 219, 254}\n",
            "Term 'class=' appears in documents: {256, 2, 661, 537, 674, 817, 450, 73, 336, 254, 214, 119, 986, 219, 226, 743, 1001, 875, 876, 754, 887, 126}\n",
            "Term 'a-link-normal' appears in documents: {674, 226, 2, 743, 1001, 875, 336, 754, 661, 119, 219, 254}\n",
            "Term 'href=' appears in documents: {674, 226, 2, 743, 1001, 875, 336, 754, 661, 119, 219, 254}\n",
            "Term '/on-stage-my550-microphone-extension-attachment-bar/dp/b0002zo3lk/ref=cm_cr_arp_d_rvw_txt' appears in documents: {1001, 2}\n",
            "Term 'ie=utf8' appears in documents: {674, 226, 2, 743, 1001, 875, 336, 754, 661, 119, 219, 254}\n",
            "Term 'stage' appears in documents: {929, 2, 389, 585, 1001, 203, 205, 463, 754, 211, 691, 755, 22, 186, 637, 990}\n",
            "Term 'my550' appears in documents: {1001, 2}\n",
            "Term 'microphone' appears in documents: {2, 263, 393, 394, 530, 914, 533, 789, 920, 154, 668, 541, 674, 807, 553, 170, 48, 437, 959, 65, 73, 201, 458, 716, 590, 596, 476, 609, 360, 1001, 619, 108, 754, 888}\n",
            "Term 'extension' appears in documents: {1001, 466, 2, 82}\n",
            "Term 'attachment' appears in documents: {1001, 2}\n",
            "Term 'bar' appears in documents: {2, 580, 357, 1001, 275, 692, 310, 822, 249, 954}\n",
            "Term '/a' appears in documents: {674, 226, 2, 743, 1001, 875, 336, 754, 661, 119, 219, 254}\n",
            "Term 'mount' appears in documents: {2, 775, 263, 138, 12, 140, 657, 789, 406, 539, 541, 164, 807, 170, 44, 689, 697, 455, 83, 471, 97, 739, 357, 1001, 240, 631, 377, 762, 892}\n",
            "Term 'mics' appears in documents: {2, 164, 324, 58, 39, 743, 1001, 619, 715, 402, 83, 310, 762, 924}\n",
            "Term 'come' appears in documents: {2, 654, 920, 408, 921, 798, 927, 799, 46, 178, 562, 823, 55, 321, 705, 451, 708, 197, 711, 73, 973, 81, 978, 851, 599, 862, 614, 504, 1001, 363, 752, 371, 500, 885, 887, 888, 126}\n",
            "Term '/nady-dmk-5-drum-mic-package/dp/b0002gxumw/ref=cm_cr_arp_d_rvw_txt' appears in documents: {1001, 2}\n",
            "Term 'nady' appears in documents: {1001, 2}\n",
            "Term 'dmk-5' appears in documents: {1001, 2}\n",
            "Term 'drum' appears in documents: {2, 775, 907, 788, 25, 411, 935, 680, 702, 839, 460, 719, 347, 864, 226, 995, 873, 1001, 619, 749, 120, 508, 382}\n",
            "Term 'mic' appears in documents: {2, 265, 393, 12, 528, 914, 789, 407, 920, 408, 668, 541, 416, 417, 803, 808, 553, 170, 44, 687, 432, 49, 689, 693, 182, 310, 312, 825, 58, 955, 316, 572, 959, 65, 450, 324, 328, 73, 458, 587, 76, 715, 590, 976, 83, 723, 855, 729, 864, 97, 611, 230, 359, 1001, 108, 754, 501, 888, 762, 508, 127}\n",
            "Term 'package' appears in documents: {2, 259, 839, 455, 393, 266, 331, 523, 745, 905, 207, 1001, 49, 370, 435, 626, 473, 475}\n",
            "Term 'relatively' appears in documents: {2, 1001, 562, 788, 437, 542}\n",
            "Term 'small' appears in documents: {2, 523, 527, 16, 537, 34, 40, 46, 572, 69, 71, 80, 594, 600, 97, 631, 637, 138, 652, 146, 662, 154, 673, 676, 177, 691, 692, 702, 704, 709, 205, 735, 747, 754, 248, 760, 762, 254, 256, 769, 781, 789, 281, 797, 292, 817, 818, 823, 830, 327, 354, 870, 360, 872, 873, 363, 376, 385, 900, 904, 395, 910, 399, 407, 412, 413, 430, 436, 442, 445, 448, 453, 456, 971, 463, 976, 985, 1001}\n",
            "Term 'light' appears in documents: {2, 646, 519, 264, 776, 778, 395, 914, 19, 790, 154, 27, 667, 670, 416, 801, 548, 932, 679, 936, 810, 557, 814, 48, 50, 179, 308, 692, 183, 568, 443, 446, 707, 69, 838, 586, 80, 721, 338, 214, 343, 221, 478, 990, 739, 228, 1001, 363, 747, 365, 493, 496, 369, 625, 755, 372, 503, 507, 381}\n",
            "Term 'microphones' appears in documents: {2, 617, 1001, 394, 976, 435, 596, 312, 729, 925, 126}\n",
            "Term 'especially' appears in documents: {2, 4, 518, 775, 137, 402, 404, 918, 157, 544, 676, 165, 807, 936, 824, 59, 63, 192, 324, 205, 345, 1001, 1003, 367, 624, 374, 760, 510}\n",
            "Term '/nady-dm70-drum-and-instrument-microphone/dp/b0002czqoq/ref=cm_cr_arp_d_rvw_txt' appears in documents: {1001, 2}\n",
            "Term 'dm70' appears in documents: {1001, 2}\n",
            "Term 'instrument' appears in documents: {2, 650, 907, 525, 785, 23, 409, 157, 551, 426, 811, 177, 434, 695, 316, 317, 444, 577, 581, 71, 845, 975, 473, 349, 478, 865, 1001, 491, 494, 241, 885, 507}\n",
            "Term 'see' appears in documents: {2, 514, 5, 28, 34, 558, 49, 564, 568, 577, 66, 593, 83, 596, 87, 600, 94, 101, 105, 624, 113, 626, 631, 656, 145, 673, 164, 690, 182, 190, 708, 197, 710, 200, 714, 204, 213, 214, 729, 731, 221, 222, 740, 745, 759, 784, 787, 279, 286, 798, 799, 803, 807, 809, 822, 840, 842, 845, 846, 849, 341, 343, 862, 864, 868, 357, 374, 377, 894, 384, 393, 395, 918, 408, 922, 415, 416, 927, 928, 420, 934, 941, 430, 945, 451, 452, 965, 458, 460, 973, 976, 467, 472, 995, 1001, 496, 503}\n",
            "Term 'customer-supplied' appears in documents: {1001, 2}\n",
            "Term 'images' appears in documents: {2, 1001, 875, 694, 95}\n",
            "Term 'top' appears in documents: {2, 646, 134, 907, 910, 17, 274, 659, 921, 926, 673, 801, 932, 934, 553, 682, 941, 430, 558, 182, 697, 954, 827, 445, 830, 65, 834, 707, 584, 840, 75, 331, 845, 718, 975, 209, 214, 600, 89, 477, 734, 97, 483, 356, 867, 615, 1001, 491, 493, 879, 113, 373}\n",
            "Term 'page' appears in documents: {128, 2, 1001, 533, 927}\n",
            "Term 'gooseneck' appears in documents: {2, 1001, 619, 759, 541}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "target_dir = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessed_Files'\n",
        "\n",
        "\n",
        "preprocessed_files = sorted(os.listdir(target_dir))[:10]  #\n",
        "\n",
        "\n",
        "for filename in preprocessed_files:\n",
        "    file_path = os.path.join(target_dir, filename)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "        print(f\"Content of {filename}:\\n{content}\\n---\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr_nUuQ7LdtH",
        "outputId": "be72ca74-6a8c-441e-e24c-438680f43931"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content of file1.txt:\n",
            "loving vintage springs vintage strat good tension great stability floating bridge want springs way go\n",
            "---\n",
            "\n",
            "Content of file10.txt:\n",
            "awesome stand tip bottom part supports guitar weird angle arrived making guitar slide back becoming almost 100 vertical solve assembled product put pressure support frame making bend little guitar sits perfectly check photos\n",
            "---\n",
            "\n",
            "Content of file100.txt:\n",
            "amp real deal great crunch gain tones tweaking half bad clean '' ish '' tones 've played two 8 '' orange cabs get cool cute crazy money sound pleasing revealing practice amp primarily play blackstar stack 've fitted celestion v30s ... wow ... is~ would never know thing tone monster ... even knobs 's easy get lost hours playing thing favorite match chapman ml-1 hotrod ... volume `` tone '' control evh fans get lot mucking around many knobs many options either guitar amp ... tone go see micro dark came ... 's probably next~ higher gain buffered effects loop speaker emu headphone recording direct 's thing\n",
            "---\n",
            "\n",
            "Content of file101.txt:\n",
            "lot mixer great podcasting 4 outputs used monitor record cue audio ... mute 3/4 figure every channel fantastic three source switch headphone/control room must podcasting also aux return inputs used extra stereo inputs volumed aux return knobs thing n't like mixer xlr outputs back require adaptors use rca 1/4 plugs get adaptors\n",
            "---\n",
            "\n",
            "Content of file102.txt:\n",
            "div id= '' video-block-r2voq5cbzhfckl '' class= '' a-section a-spacing-small a-spacing-top-mini video-block '' /div input type= '' hidden '' name= '' '' value= '' https //images-na.ssl-images-amazon.com/images/i/e1 2b6mhk2mfs.mp4 '' class= '' video-url '' input type= '' hidden '' name= '' '' value= '' https //images-na.ssl-images-amazon.com/images/i/21-jk5lxqss.png '' class= '' video-slate-img-url '' nbsp mic boss lot better mic 've seen used out-of-the-box voice sounds great even processing compression eq sounds fantastic rejects ton background noise sounds amazing runs hot 'll want clean pre-amping get clean signal amazing mic price\n",
            "---\n",
            "\n",
            "Content of file103.txt:\n",
            "practical versatile instrument tuner includes metronome tone generator variety settings clamp attaches easily instrument tuner rotate angle convenient seeing power function buttons easy use limitations frequency range limit 435-445 hz meter shows flat sharp right note precise tuner wo n't get green light slightly tried multiple tuners one good extra features make great value go accessory musicians received sample cost honest unbiased review\n",
            "---\n",
            "\n",
            "Content of file104.txt:\n",
            "purchased cigar box guitar work great purpose complaints\n",
            "---\n",
            "\n",
            "Content of file105.txt:\n",
            "div id= '' video-block-r21fu14f1rfdce '' class= '' a-section a-spacing-small a-spacing-top-mini video-block '' /div input type= '' hidden '' name= '' '' value= '' https //images-na.ssl-images-amazon.com/images/i/b1hpwitr57s.mp4 '' class= '' video-url '' input type= '' hidden '' name= '' '' value= '' https //images-na.ssl-images-amazon.com/images/i/a1vfa9m6was.png '' class= '' video-slate-img-url '' nbsp set 2 audio 2000s heavy-duty quality microphones case two came durable microphones clips securely padded within case makes transporting breeze tested worked perfectly fine pa. microphones sounded clean bass ier side adjusted desired overall great buy really awesome come set nice hard case action work really well 100 could used professional setting thanks audio 2000s disclaimer received product discounted rate exchange honest unbiased review\n",
            "---\n",
            "\n",
            "Content of file106.txt:\n",
            "dean guitars disappoint finish perfect least mine grover tuners stay tune gibson sg 's price low sound quality tolerance construction low action balanced feel neck abalone dot inlays made look abalone pickups good go height maybe adjusted semi hollow sounds rule adds depth character two dean guitars finish quality wood still active guitar angled bolt-on neck loss binding quality nothing even close beautiful look play\n",
            "---\n",
            "\n",
            "Content of file107.txt:\n",
            "case bought epiphone les paul black beauty 3 fits inside pretty well although little wiggle room case appears built well however case lid misaligned base section closed shut lid overlap bottom case similar shoe box lid fits top box misalignment creates odd gap noticeable upon close inspection similar epiphone case sheraton case closes properly like shoe box example beware production quality may consistant could vary case case basis could n't resist pun also feel guitar fit snugly inside case similar gretsch g5120 electromatic fits inside gretsch g6241ft flat top case gretsch g5120 snug almost tight fit inside g6241ft case allows absolutely wiggle room aside two aforementioned complaints generally satisfied epiphone case provides adequate shelter les paul use\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove punctuation from each token\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "\n",
        "    # Remove remaining tokens that are not alphabetic\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "\n",
        "    # Filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "\n",
        "    # Remove blank space tokens, if any\n",
        "    words = [w for w in words if w.strip()]\n",
        "\n",
        "    return words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUAg3Z4Berwx",
        "outputId": "acc9bcfd-62d2-4de0-d42e-ee2074975518"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the boolean operation functions and loaded_inverted_index are defined\n",
        "\n",
        "def process_queries(queries, loaded_inverted_index, universe_set):\n",
        "    for i, (terms_str, ops_str) in enumerate(queries, start=1):\n",
        "        terms = terms_str.lower().split()\n",
        "        ops = ops_str.split(\", \")\n",
        "\n",
        "        # Initialize the result set with the first term's document IDs\n",
        "        result_set = loaded_inverted_index.get(terms[0], set())\n",
        "        for op, term in zip(ops, terms[1:]):\n",
        "            next_set = loaded_inverted_index.get(term, set())\n",
        "            if op == \"AND\":\n",
        "                result_set = query_and(result_set, next_set)\n",
        "            elif op == \"OR\":\n",
        "                result_set = query_or(result_set, next_set)\n",
        "            elif op == \"AND NOT\":\n",
        "                result_set = query_and_not(result_set, next_set)\n",
        "            elif op == \"OR NOT\":\n",
        "                result_set = query_or_not(result_set, next_set, universe_set)\n",
        "\n",
        "        # Output the formatted results\n",
        "        print(f\"Query {i}: {' '.join(terms)} {' '.join(ops)}\")\n",
        "        print(f\"Number of documents retrieved for query {i}: {len(result_set)}\")\n",
        "        doc_names = sorted([f\"{doc_id}\" for doc_id in result_set])  # Assuming doc_id are filenames\n",
        "        print(f\"Names of the documents retrieved for query {i}: {', '.join(doc_names)}\\n\")\n",
        "\n",
        "# Sample queries for demonstration (replace with actual queries)\n",
        "# sample_queries = [\n",
        "#     (preprocess_text(\"Car bag in a canister\"), \"OR, AND NOT\"),\n",
        "#     (preprocess_text(\"Coffee brewing techniques in cookbook\"), \"AND, OR NOT, OR\")\n",
        "# ]\n",
        "\n",
        "# Process the sample queries (assuming loaded_inverted_index and universe_set are defined)\n",
        "# process_queries(sample_queries, loaded_inverted_index, universe_set)\n",
        "\n",
        "\n",
        "sample_queries = [\n",
        "    \"Car bag in a canister\",\n",
        "    \"Coffee brewing techniques in cookbook\"\n",
        "]\n",
        "\n",
        "# # Preprocess each query\n",
        "# preprocessed_queries = [preprocess_text(query) for query in sample_queries]\n",
        "# #print(preprocessed_queries)\n",
        "\n"
      ],
      "metadata": {
        "id": "BmLhbdxReldO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import pickle\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in stripped if word.isalpha() and word not in stop_words]\n",
        "    return words\n",
        "\n",
        "def process_query(query_str, ops_str, loaded_inverted_index, universe_set):\n",
        "    terms = preprocess_text(query_str)\n",
        "    ops = ops_str.split(\", \")\n",
        "    result_set = set(loaded_inverted_index.get(terms[0], set()))\n",
        "    query_representation = terms[0]\n",
        "\n",
        "    for op, term in zip(ops, terms[1:]):\n",
        "        next_set = set(loaded_inverted_index.get(term, set()))\n",
        "        if op == \"AND\":\n",
        "            result_set &= next_set\n",
        "        elif op == \"OR\":\n",
        "            result_set |= next_set\n",
        "        elif op == \"AND NOT\":\n",
        "            result_set -= next_set\n",
        "        elif op == \"OR NOT\":\n",
        "            result_set |= (universe_set - next_set)\n",
        "        query_representation += f\" {op} {term}\"\n",
        "\n",
        "    doc_names = sorted([f\"{doc_id}.txt\" for doc_id in result_set])\n",
        "    return query_representation, len(result_set), doc_names\n",
        "\n",
        "def user_input_and_search(loaded_inverted_index, universe_set):\n",
        "    num_queries = int(input(\"Enter number of queries: \"))\n",
        "    for i in range(num_queries):\n",
        "        query_str = input(f\"Enter query {i+1}: \")\n",
        "        ops_str = input(f\"Enter operations for query {i+1} (comma-separated): \")\n",
        "        query_representation, num_docs, doc_names = process_query(query_str, ops_str, loaded_inverted_index, universe_set)\n",
        "\n",
        "        print(f\"Query {i+1}: {query_representation}\")\n",
        "        print(f\"Number of documents retrieved for query {i+1}: {num_docs}\")\n",
        "        if doc_names:\n",
        "            print(f\"Names of the documents retrieved for query {i+1}: {', '.join(doc_names)}\")\n",
        "        else:\n",
        "            print(\"No documents found.\")\n",
        "        print()\n",
        "\n",
        "\n",
        "\n",
        "# Calling  the user input and search function with the loaded data\n",
        "user_input_and_search(loaded_inverted_index, universe_set)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfL8hW8YyhxT",
        "outputId": "aae6f683-0063-4406-95d3-e08e0628ce65"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of queries: 1\n",
            "Enter query 1: Car bag in a canister\n",
            "Enter operations for query 1 (comma-separated): OR, AND NOT\n",
            "Query 1: car OR bag AND NOT canister\n",
            "Number of documents retrieved for query 1: 31\n",
            "Names of the documents retrieved for query 1: 125.txt, 14.txt, 154.txt, 172.txt, 231.txt, 254.txt, 266.txt, 28.txt, 304.txt, 319.txt, 38.txt, 402.txt, 423.txt, 430.txt, 474.txt, 513.txt, 562.txt, 62.txt, 673.txt, 69.txt, 691.txt, 74.txt, 759.txt, 772.txt, 822.txt, 885.txt, 896.txt, 899.txt, 907.txt, 93.txt, 934.txt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def process_queries(queries, loaded_inverted_index, universe_set):\n",
        "#     results = []\n",
        "#     for query_index, (query_str, ops_str) in enumerate(queries, start=1):\n",
        "#         terms = preprocess_text(query_str)\n",
        "#         ops = ops_str.split(\", \")\n",
        "\n",
        "#         result_set = set(loaded_inverted_index.get(terms[0], []))\n",
        "#         query_representation = [terms[0]]  # Initialize with the first term\n",
        "\n",
        "#         for term, op in zip(terms[1:], ops):\n",
        "#             next_set = set(loaded_inverted_index.get(term, []))\n",
        "#             if op == \"AND\":\n",
        "#                 result_set &= next_set\n",
        "#                 query_representation.append(\"AND \" + term)\n",
        "#             elif op == \"OR\":\n",
        "#                 result_set |= next_set\n",
        "#                 query_representation.append(\"OR \" + term)\n",
        "#             elif op == \"AND NOT\":\n",
        "#                 result_set -= next_set\n",
        "#                 query_representation.append(\"AND NOT \" + term)\n",
        "#             elif op == \"OR NOT\":\n",
        "#                 result_set |= (universe_set - next_set)\n",
        "#                 query_representation.append(\"OR NOT \" + term)\n",
        "\n",
        "#         formatted_query = ' '.join(query_representation)\n",
        "#         doc_names = sorted([str(doc_id) for doc_id in result_set])\n",
        "#         results.append((formatted_query, len(result_set), doc_names))\n",
        "#     return results\n"
      ],
      "metadata": {
        "id": "fCTBUR4uNMKL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t-DXKyu_MR2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **# Q3. Positional Index and Phrase Queries**"
      ],
      "metadata": {
        "id": "mPNzDzpxO0yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "source_dir = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessed_Files'\n",
        "\n",
        "\n",
        "positional_index = {}\n",
        "\n",
        "for filename in os.listdir(source_dir):\n",
        "    file_path = os.path.join(source_dir, filename)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "        words = content.split()\n",
        "        for position, word in enumerate(words):\n",
        "            if word not in positional_index:\n",
        "                positional_index[word] = {}\n",
        "            if filename not in positional_index[word]:\n",
        "                positional_index[word][filename] = []\n",
        "            positional_index[word][filename].append(position)\n"
      ],
      "metadata": {
        "id": "Me5gRrlsPb8V"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#path for saving the positional index\n",
        "index_file_path = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/positional_index.pkl'\n",
        "\n",
        "# Serializing and saving  the positional index\n",
        "with open(index_file_path, 'wb') as index_file:\n",
        "    pickle.dump(positional_index, index_file)\n",
        "\n",
        "print(\"Positional index saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuzaRyhVQhQ4",
        "outputId": "5914d4fb-11b7-4e72-b72a-25d815a0c12c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positional index saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Positional Index as per the document.\n",
        "with open(index_file_path, 'rb') as index_file:\n",
        "    loaded_positional_index = pickle.load(index_file)\n",
        "\n",
        "print(\"Positional index loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "WlmHdlvSQl64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example code to inspect the positional index for specific terms\n",
        "terms_to_check = ['car', 'coffee']\n",
        "for term in terms_to_check:\n",
        "    if term in loaded_positional_index:\n",
        "        print(f\"Term '{term}' is in the index.\")\n",
        "    else:\n",
        "        print(f\"Term '{term}' is not found in the index.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ4qiyzWW-Gt",
        "outputId": "1e5329ad-f0d1-48b3-e9ba-a2f2691143c5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term 'car' is in the index.\n",
            "Term 'coffee' is in the index.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_query(query):\n",
        "    # Assuming more sophisticated preprocessing if necessary\n",
        "    query = query.lower()  # Lowercase the query\n",
        "    tokens = query.split()  # Tokenization - assuming space-separated words\n",
        "    # Include additional preprocessing steps here if they were applied in Q1\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "PDPclAp_YvJs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Path to your serialized positional index\n",
        "index_file_path = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/positional_index.pkl'\n",
        "\n",
        "# Load the positional index from the file\n",
        "with open(index_file_path, 'rb') as index_file:\n",
        "    positional_index = pickle.load(index_file)\n",
        "\n",
        "# Print the first 50 entries of the positional index\n",
        "for i, (term, doc_positions) in enumerate(positional_index.items()):\n",
        "    print(f\"Term '{term}' appears in documents and positions: {doc_positions}\")\n",
        "    if i >= 49:  # Stop after printing 50 entries\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm63i2eAZ4S3",
        "outputId": "d96f1321-a4c3-40e9-b171-fc72c08beaa4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term 'first' appears in documents and positions: {'file728.txt': [0], 'file365.txt': [4], 'file736.txt': [35], 'file542.txt': [23, 43], 'file487.txt': [14], 'file956.txt': [33], 'file708.txt': [0], 'file638.txt': [0], 'file240.txt': [44], 'file448.txt': [24], 'file992.txt': [48], 'file758.txt': [9], 'file575.txt': [50], 'file23.txt': [20], 'file480.txt': [0], 'file280.txt': [13], 'file912.txt': [64], 'file216.txt': [7], 'file222.txt': [0], 'file336.txt': [3], 'file651.txt': [7], 'file774.txt': [0], 'file821.txt': [24], 'file702.txt': [38], 'file689.txt': [30], 'file303.txt': [24], 'file341.txt': [12], 'file524.txt': [78], 'file541.txt': [18], 'file127.txt': [3], 'file911.txt': [56], 'file778.txt': [11], 'file576.txt': [0, 23], 'file89.txt': [67], 'file681.txt': [42], 'file443.txt': [27, 46], 'file358.txt': [26], 'file990.txt': [14], 'file550.txt': [22, 41], 'file591.txt': [0], 'file743.txt': [15], 'file59.txt': [38], 'file213.txt': [0], 'file404.txt': [9], 'file801.txt': [77], 'file621.txt': [0], 'file271.txt': [6, 10], 'file133.txt': [0], 'file522.txt': [0], 'file632.txt': [34], 'file839.txt': [45], 'file968.txt': [0], 'file467.txt': [0, 94], 'file469.txt': [6], 'file202.txt': [39], 'file878.txt': [4], 'file687.txt': [3], 'file692.txt': [20], 'file519.txt': [41], 'file42.txt': [3], 'file676.txt': [12], 'file325.txt': [8], 'file254.txt': [49], 'file19.txt': [49], 'file927.txt': [43], 'file163.txt': [35], 'file729.txt': [8], 'file166.txt': [66], 'file930.txt': [11], 'file706.txt': [11], 'file129.txt': [17], 'file744.txt': [14], 'file858.txt': [39], 'file453.txt': [0], 'file430.txt': [8, 17], 'file225.txt': [15]}\n",
            "Term 'pulled' appears in documents and positions: {'file728.txt': [1], 'file382.txt': [29], 'file77.txt': [13], 'file573.txt': [16], 'file641.txt': [4], 'file670.txt': [24], 'file947.txt': [26], 'file325.txt': [48], 'file298.txt': [6]}\n",
            "Term 'box' appears in documents and positions: {'file728.txt': [2], 'file372.txt': [39, 56], 'file343.txt': [52], 'file942.txt': [2], 'file382.txt': [25], 'file460.txt': [34], 'file251.txt': [6], 'file107.txt': [34, 38, 56], 'file396.txt': [35], 'file176.txt': [4], 'file719.txt': [47], 'file313.txt': [24], 'file248.txt': [2], 'file850.txt': [37], 'file104.txt': [2], 'file183.txt': [25], 'file334.txt': [14], 'file828.txt': [10], 'file314.txt': [37], 'file578.txt': [0, 50], 'file694.txt': [6], 'file279.txt': [65], 'file191.txt': [3], 'file986.txt': [13], 'file30.txt': [42], 'file870.txt': [2], 'file438.txt': [17, 19, 21, 24], 'file522.txt': [5], 'file616.txt': [12, 22], 'file332.txt': [7], 'file676.txt': [63], 'file166.txt': [49], 'file706.txt': [41], 'file189.txt': [4, 11]}\n",
            "Term 'pleasantly' appears in documents and positions: {'file728.txt': [3], 'file736.txt': [18], 'file448.txt': [32], 'file264.txt': [22], 'file946.txt': [3], 'file326.txt': [8], 'file183.txt': [10], 'file84.txt': [17], 'file160.txt': [20], 'file990.txt': [2], 'file61.txt': [9], 'file543.txt': [25], 'file748.txt': [92], 'file79.txt': [66], 'file652.txt': [4], 'file527.txt': [0]}\n",
            "Term 'surprised' appears in documents and positions: {'file728.txt': [4], 'file736.txt': [19], 'file487.txt': [13], 'file448.txt': [33], 'file571.txt': [17], 'file264.txt': [23], 'file414.txt': [49], 'file943.txt': [0], 'file946.txt': [4], 'file441.txt': [1], 'file569.txt': [48], 'file326.txt': [9], 'file183.txt': [11], 'file84.txt': [18], 'file273.txt': [12], 'file160.txt': [21], 'file747.txt': [6], 'file990.txt': [3], 'file40.txt': [0], 'file461.txt': [56], 'file202.txt': [9, 13], 'file61.txt': [10], 'file543.txt': [26], 'file230.txt': [28], 'file309.txt': [2], 'file748.txt': [93], 'file79.txt': [67], 'file652.txt': [5], 'file865.txt': [15], 'file64.txt': [66], 'file527.txt': [1]}\n",
            "Term 'quite' appears in documents and positions: {'file728.txt': [5], 'file224.txt': [27], 'file174.txt': [64], 'file412.txt': [14], 'file4.txt': [5], 'file11.txt': [63], 'file344.txt': [21], 'file678.txt': [9], 'file648.txt': [8], 'file326.txt': [56], 'file824.txt': [1], 'file96.txt': [31], 'file974.txt': [26], 'file28.txt': [81], 'file746.txt': [33], 'file640.txt': [29], 'file307.txt': [6], 'file514.txt': [25], 'file591.txt': [33], 'file906.txt': [5], 'file130.txt': [3], 'file637.txt': [44], 'file462.txt': [19], 'file136.txt': [8], 'file989.txt': [39], 'file687.txt': [48], 'file141.txt': [30], 'file521.txt': [77], 'file671.txt': [0], 'file22.txt': [15]}\n",
            "Term 'negative' appears in documents and positions: {'file728.txt': [6], 'file33.txt': [41], 'file631.txt': [39], 'file324.txt': [55], 'file848.txt': [93], 'file650.txt': [19], 'file721.txt': [8], 'file544.txt': [18, 53]}\n",
            "Term 'reviews' appears in documents and positions: {'file728.txt': [7], 'file253.txt': [1], 'file372.txt': [4], 'file343.txt': [88], 'file33.txt': [42], 'file200.txt': [3], 'file382.txt': [24], 'file172.txt': [65], 'file867.txt': [25], 'file243.txt': [5], 'file719.txt': [2], 'file313.txt': [22], 'file311.txt': [47], 'file312.txt': [18], 'file324.txt': [20], 'file208.txt': [21], 'file906.txt': [1], 'file137.txt': [12], 'file483.txt': [1], 'file621.txt': [45], 'file896.txt': [18], 'file21.txt': [8], 'file915.txt': [5], 'file766.txt': [3], 'file214.txt': [1], 'file748.txt': [62], 'file497.txt': [40], 'file429.txt': [24], 'file294.txt': [5], 'file19.txt': [3], 'file675.txt': [21], 'file544.txt': [19], 'file337.txt': [33], 'file890.txt': [44]}\n",
            "Term 'half' appears in documents and positions: {'file728.txt': [8], 'file951.txt': [21], 'file887.txt': [2], 'file805.txt': [68], 'file89.txt': [7], 'file629.txt': [21, 25], 'file497.txt': [15], 'file100.txt': [8], 'file430.txt': [21, 28]}\n",
            "Term 'expecting' appears in documents and positions: {'file728.txt': [9], 'file760.txt': [13], 'file188.txt': [0, 21], 'file565.txt': [19], 'file216.txt': [22], 'file469.txt': [2], 'file827.txt': [9], 'file961.txt': [13]}\n",
            "Term 'arrive' appears in documents and positions: {'file728.txt': [10], 'file742.txt': [48], 'file565.txt': [18], 'file408.txt': [47], 'file293.txt': [38], 'file226.txt': [14], 'file179.txt': [2]}\n",
            "Term 'pieces' appears in documents and positions: {'file728.txt': [11], 'file412.txt': [43], 'file477.txt': [25], 'file366.txt': [11], 'file928.txt': [68], 'file915.txt': [73], 'file684.txt': [11], 'file686.txt': [25, 53], 'file54.txt': [28]}\n",
            "Term 'packaged' appears in documents and positions: {'file728.txt': [12], 'file251.txt': [2], 'file586.txt': [29], 'file512.txt': [37], 'file706.txt': [56], 'file674_preprocessed.txt': [16]}\n",
            "Term 'well' appears in documents and positions: {'file728.txt': [13], 'file709.txt': [30], 'file542.txt': [46], 'file224.txt': [1, 6, 28], 'file352.txt': [30], 'file956.txt': [21], 'file742.txt': [61], 'file708.txt': [29], 'file888.txt': [17], 'file771.txt': [9], 'file808.txt': [13], 'file638.txt': [91], 'file240.txt': [4], 'file347.txt': [47], 'file603.txt': [21], 'file942.txt': [16], 'file593.txt': [36], 'file448.txt': [82], 'file126.txt': [60], 'file446.txt': [19], 'file940.txt': [11], 'file283.txt': [21], 'file953.txt': [54], 'file992.txt': [32, 43, 51], 'file477.txt': [16], 'file345.txt': [10], 'file760.txt': [15, 79], 'file649.txt': [24], 'file490.txt': [20], 'file105.txt': [94], 'file65.txt': [61], 'file366.txt': [41], 'file727.txt': [5, 12], 'file425.txt': [12, 30], 'file946.txt': [1, 27], 'file239.txt': [0], 'file575.txt': [32], 'file284.txt': [8], 'file379.txt': [4], 'file798.txt': [21, 23], 'file643.txt': [3], 'file751.txt': [4], 'file251.txt': [1, 20], 'file117.txt': [8], 'file172.txt': [2], 'file875.txt': [1], 'file572.txt': [0], 'file107.txt': [11, 19], 'file609.txt': [20], 'file216.txt': [52, 68], 'file775.txt': [12, 27], 'file58.txt': [6], 'file336.txt': [45], 'file586.txt': [30], 'file384.txt': [29], 'file396.txt': [31], 'file569.txt': [57], 'file377.txt': [14], 'file651.txt': [39], 'file362.txt': [59], 'file529.txt': [0], 'file861.txt': [10, 18], 'file702.txt': [15], 'file780.txt': [38], 'file354.txt': [4], 'file979.txt': [46], 'file308.txt': [0], 'file618.txt': [8], 'file523.txt': [33, 59], 'file682.txt': [3], 'file436.txt': [34, 35], 'file395.txt': [15], 'file70.txt': [13], 'file35.txt': [20, 51, 76], 'file323.txt': [6], 'file312.txt': [63], 'file899.txt': [8], 'file84.txt': [81], 'file324.txt': [43], 'file536.txt': [8, 19], 'file28.txt': [17], 'file667.txt': [11], 'file805.txt': [37], 'file955.txt': [8], 'file361.txt': [31], 'file787.txt': [6], 'file898.txt': [27, 38], 'file860.txt': [1, 8], 'file907.txt': [48], 'file339.txt': [56], 'file604.txt': [22], 'file120.txt': [41], 'file279.txt': [60], 'file596.txt': [21], 'file358.txt': [10], 'file512.txt': [18, 40], 'file846.txt': [31], 'file99.txt': [41], 'file990.txt': [6], 'file958.txt': [1], 'file591.txt': [34], 'file906.txt': [6], 'file40.txt': [13], 'file877.txt': [11], 'file368.txt': [2], 'file743.txt': [29, 31], 'file290.txt': [22], 'file659.txt': [0], 'file130.txt': [4], 'file833.txt': [35], 'file716.txt': [14], 'file994.txt': [12], 'file30.txt': [35], 'file731.txt': [49], 'file777.txt': [64], 'file560.txt': [6], 'file801.txt': [73], 'file418.txt': [17], 'file715.txt': [11], 'file292.txt': [85], 'file619.txt': [6], 'file45.txt': [65], 'file611.txt': [31], 'file133.txt': [17], 'file501.txt': [4], 'file522.txt': [54], 'file632.txt': [26], 'file935.txt': [7, 16, 64], 'file616.txt': [43], 'file580.txt': [3], 'file857.txt': [7], 'file246.txt': [34], 'file317.txt': [12], 'file118.txt': [10], 'file989.txt': [24], 'file564.txt': [0], 'file489.txt': [21], 'file796.txt': [52, 68], 'file3.txt': [49], 'file884.txt': [11, 65], 'file90.txt': [9, 11], 'file553.txt': [53], 'file605.txt': [12], 'file135.txt': [18, 20], 'file332.txt': [23], 'file615.txt': [7], 'file228.txt': [93], 'file145.txt': [2], 'file819.txt': [7], 'file434.txt': [5], 'file838.txt': [17], 'file543.txt': [33], 'file868.txt': [29], 'file27.txt': [3], 'file827.txt': [44], 'file419.txt': [15], 'file978.txt': [3], 'file629.txt': [43], 'file309.txt': [3], 'file320.txt': [14], 'file769.txt': [46], 'file810.txt': [30], 'file355.txt': [82], 'file652.txt': [27], 'file221.txt': [15], 'file148.txt': [7, 14], 'file862.txt': [13], 'file429.txt': [30], 'file406.txt': [32], 'file392.txt': [3], 'file325.txt': [55, 91, 93], 'file835.txt': [58], 'file679.txt': [4], 'file672.txt': [77], 'file865.txt': [16], 'file673.txt': [31], 'file329.txt': [7], 'file166.txt': [81], 'file566.txt': [20], 'file706.txt': [46], 'file466.txt': [9], 'file444.txt': [44], 'file305.txt': [10], 'file51.txt': [6], 'file455.txt': [5, 29], 'file527.txt': [14], 'file227.txt': [1, 13], 'file430.txt': [48], 'file773.txt': [10], 'file598.txt': [31], 'file211.txt': [46], 'file470.txt': [9], 'file390.txt': [85], 'file81.txt': [1], 'file674_preprocessed.txt': [15]}\n",
            "Term 'already' appears in documents and positions: {'file728.txt': [14], 'file174.txt': [41], 'file603.txt': [31], 'file65.txt': [6], 'file510.txt': [7], 'file920.txt': [31], 'file588.txt': [44], 'file689.txt': [24], 'file626.txt': [43], 'file907.txt': [6], 'file786.txt': [14], 'file386.txt': [19], 'file910.txt': [18, 36], 'file766.txt': [11], 'file796.txt': [66], 'file258.txt': [24], 'file748.txt': [72], 'file656.txt': [18], 'file129.txt': [59], 'file825.txt': [36], 'file918.txt': [29], 'file890.txt': [21]}\n",
            "Term 'assembled' appears in documents and positions: {'file728.txt': [15], 'file458.txt': [23], 'file10.txt': [19]}\n",
            "Term 'pretty' appears in documents and positions: {'file728.txt': [16, 46, 53], 'file73.txt': [16], 'file412.txt': [31], 'file760.txt': [49, 81], 'file533.txt': [15], 'file239.txt': [63], 'file85.txt': [4], 'file643.txt': [5], 'file710.txt': [11], 'file422.txt': [0], 'file739.txt': [3], 'file474.txt': [9], 'file168.txt': [15], 'file107.txt': [10], 'file216.txt': [42], 'file633.txt': [17], 'file588.txt': [24], 'file618.txt': [37], 'file28.txt': [71], 'file911.txt': [38], 'file576.txt': [19], 'file694.txt': [0], 'file863.txt': [17], 'file514.txt': [48], 'file591.txt': [25], 'file743.txt': [23], 'file378.txt': [0], 'file637.txt': [29], 'file502.txt': [14], 'file870.txt': [38], 'file777.txt': [4], 'file404.txt': [45], 'file980.txt': [44], 'file801.txt': [72], 'file338.txt': [17, 23, 32], 'file632.txt': [45], 'file580.txt': [7], 'file796.txt': [53], 'file884.txt': [1], 'file90.txt': [27], 'file214.txt': [28], 'file228.txt': [82], 'file519.txt': [24, 38, 47], 'file811.txt': [21], 'file112.txt': [11], 'file355.txt': [89], 'file294.txt': [40], 'file193.txt': [1], 'file19.txt': [68], 'file923.txt': [2], 'file129.txt': [8], 'file189.txt': [21], 'file814.txt': [1], 'file776.txt': [18], 'file108.txt': [72], 'file50.txt': [31], 'file918.txt': [43]}\n",
            "Term 'happy' appears in documents and positions: {'file728.txt': [17], 'file756.txt': [47], 'file945.txt': [4], 'file73.txt': [64], 'file126.txt': [67], 'file250.txt': [6], 'file845.txt': [19], 'file760.txt': [94], 'file649.txt': [30], 'file562.txt': [13], 'file366.txt': [2], 'file284.txt': [0], 'file144.txt': [36], 'file545.txt': [19], 'file983.txt': [32], 'file714.txt': [12, 26], 'file631.txt': [72], 'file350.txt': [11], 'file127.txt': [25], 'file263.txt': [18], 'file799.txt': [4], 'file863.txt': [43], 'file550.txt': [35], 'file322.txt': [86], 'file241.txt': [36], 'file59.txt': [33], 'file621.txt': [10], 'file338.txt': [56], 'file292.txt': [87], 'file525.txt': [37], 'file632.txt': [46], 'file935.txt': [51, 54, 109], 'file616.txt': [30], 'file755.txt': [7], 'file551.txt': [12], 'file766.txt': [4], 'file467.txt': [38], 'file489.txt': [28, 29], 'file464.txt': [0], 'file966.txt': [1], 'file434.txt': [39, 42], 'file838.txt': [29], 'file868.txt': [34], 'file972.txt': [12], 'file680.txt': [1], 'file963.txt': [5], 'file634.txt': [27], 'file597.txt': [2], 'file650.txt': [17], 'file148.txt': [67], 'file862.txt': [35], 'file429.txt': [49], 'file193.txt': [2], 'file285.txt': [22], 'file24.txt': [38], 'file363.txt': [98], 'file923.txt': [45], 'file154.txt': [34, 42], 'file697.txt': [2], 'file527.txt': [20], 'file772.txt': [31]}\n",
            "Term 'installed' appears in documents and positions: {'file728.txt': [18], 'file542.txt': [13], 'file487.txt': [27], 'file813.txt': [37], 'file756.txt': [0], 'file278.txt': [0], 'file940.txt': [2], 'file992.txt': [4], 'file168.txt': [0], 'file439.txt': [58], 'file627.txt': [1, 41], 'file617.txt': [0], 'file123.txt': [36], 'file523.txt': [25], 'file440.txt': [31], 'file626.txt': [0, 42], 'file408.txt': [2], 'file558.txt': [24, 31], 'file353.txt': [12], 'file48.txt': [39], 'file18.txt': [21], 'file380.txt': [5], 'file340.txt': [6], 'file796.txt': [0], 'file98.txt': [23], 'file155.txt': [49], 'file644.txt': [0, 8], 'file39.txt': [17], 'file54.txt': [0], 'file355.txt': [5], 'file670.txt': [0], 'file294.txt': [20], 'file245.txt': [22], 'file163.txt': [80], 'file428.txt': [81], 'file995.txt': [0, 19], 'file108.txt': [8], 'file81.txt': [4]}\n",
            "Term 'rack' appears in documents and positions: {'file728.txt': [19], 'file487.txt': [3], 'file109.txt': [2], 'file128.txt': [1], 'file711.txt': [19], 'file397.txt': [13], 'file622.txt': [7], 'file958.txt': [8], 'file922.txt': [19, 24], 'file582.txt': [10], 'file615.txt': [18], 'file29.txt': [9], 'file769.txt': [3], 'file825.txt': [32], 'file949_preprocessed.txt': [96], 'file949.txt': [96]}\n",
            "Term 'noticed' appears in documents and positions: {'file728.txt': [20], 'file405.txt': [15], 'file945.txt': [7], 'file48.txt': [25], 'file491.txt': [51], 'file19.txt': [17], 'file64.txt': [90], 'file675.txt': [20]}\n",
            "Term 'cable' appears in documents and positions: {'file728.txt': [21], 'file372.txt': [32], 'file260.txt': [0, 14, 57], 'file223.txt': [23], 'file302.txt': [41, 49], 'file943.txt': [16], 'file572.txt': [14], 'file879.txt': [14], 'file541.txt': [63], 'file573.txt': [40, 48], 'file898.txt': [1, 16], 'file512.txt': [48], 'file99.txt': [1], 'file622.txt': [34, 37], 'file863.txt': [10], 'file94.txt': [2], 'file783.txt': [2], 'file290.txt': [31, 48], 'file624.txt': [1], 'file833.txt': [24, 28], 'file133.txt': [30], 'file501.txt': [0, 15], 'file551.txt': [10], 'file610.txt': [6], 'file467.txt': [50, 55, 87], 'file905.txt': [1, 15], 'file36.txt': [14, 19], 'file39.txt': [2, 36, 51], 'file294.txt': [72], 'file233.txt': [49], 'file17.txt': [13, 30, 35], 'file521.txt': [50], 'file375.txt': [16], 'file69.txt': [3], 'file453.txt': [79], 'file825.txt': [46]}\n",
            "Term 'holes' appears in documents and positions: {'file728.txt': [22], 'file736.txt': [37], 'file542.txt': [75, 86], 'file420.txt': [17], 'file283.txt': [5], 'file992.txt': [9, 24], 'file13.txt': [14], 'file985.txt': [4], 'file965.txt': [16], 'file685.txt': [2], 'file633.txt': [36], 'file268.txt': [12], 'file753.txt': [10], 'file737.txt': [19, 23], 'file436.txt': [6], 'file440.txt': [9, 16, 39], 'file350.txt': [9], 'file626.txt': [24, 29, 32, 38], 'file208.txt': [54], 'file922.txt': [22], 'file287.txt': [11], 'file152.txt': [14], 'file241.txt': [28, 34], 'file293.txt': [26], 'file611.txt': [26], 'file915.txt': [6], 'file718.txt': [56], 'file380.txt': [53], 'file873.txt': [6, 12], 'file834.txt': [30], 'file355.txt': [85], 'file862.txt': [24], 'file656.txt': [15, 21], 'file815.txt': [11], 'file772.txt': [5, 10, 20]}\n",
            "Term 'back' appears in documents and positions: {'file728.txt': [23, 60], 'file542.txt': [80], 'file487.txt': [20], 'file352.txt': [37], 'file813.txt': [49], 'file92.txt': [2], 'file937.txt': [68], 'file239.txt': [10], 'file575.txt': [59], 'file660.txt': [19], 'file367.txt': [7], 'file11.txt': [61], 'file460.txt': [62], 'file413.txt': [91], 'file144.txt': [17], 'file168.txt': [40], 'file633.txt': [5], 'file394.txt': [14], 'file248.txt': [24], 'file588.txt': [48], 'file861.txt': [24], 'file482.txt': [2], 'file753.txt': [7, 27], 'file476.txt': [10], 'file936.txt': [31], 'file334.txt': [34, 40, 63], 'file66.txt': [7], 'file156.txt': [5], 'file631.txt': [64], 'file778.txt': [28], 'file613.txt': [24], 'file157.txt': [23], 'file591.txt': [14, 20], 'file191.txt': [23], 'file322.txt': [47], 'file353.txt': [44], 'file994.txt': [64], 'file270.txt': [13, 35], 'file777.txt': [60], 'file534.txt': [57], 'file621.txt': [53], 'file701.txt': [63], 'file402.txt': [9], 'file522.txt': [45], 'file553.txt': [62], 'file855.txt': [3], 'file400.txt': [41], 'file827.txt': [51], 'file849.txt': [47], 'file150.txt': [49], 'file765.txt': [40], 'file101.txt': [43], 'file830.txt': [14], 'file10.txt': [13], 'file491.txt': [72], 'file770.txt': [73, 100], 'file497.txt': [10], 'file676.txt': [18, 24], 'file294.txt': [60], 'file961.txt': [47, 55], 'file815.txt': [21, 40], 'file163.txt': [72], 'file233.txt': [11], 'file166.txt': [59], 'file129.txt': [95], 'file428.txt': [62], 'file880.txt': [66], 'file47.txt': [26, 35], 'file453.txt': [18], 'file767.txt': [51], 'file22.txt': [30], 'file265.txt': [46]}\n",
            "Term 'n't' appears in documents and positions: {'file728.txt': [24, 36, 56, 61, 65], 'file304.txt': [20], 'file405.txt': [14], 'file982.txt': [21], 'file797.txt': [14], 'file709.txt': [27], 'file736.txt': [2, 4, 9, 16], 'file841.txt': [31], 'file542.txt': [2], 'file487.txt': [28, 34], 'file389.txt': [16], 'file554.txt': [30], 'file813.txt': [12, 19, 65, 88], 'file574.txt': [1], 'file142.txt': [3], 'file372.txt': [67], 'file52.txt': [7], 'file847.txt': [56], 'file742.txt': [70], 'file708.txt': [40, 78, 86], 'file888.txt': [9], 'file343.txt': [6, 20, 27, 67, 98], 'file991.txt': [24], 'file945.txt': [60], 'file174.txt': [90], 'file275.txt': [5], 'file638.txt': [75], 'file420.txt': [10, 30], 'file73.txt': [29], 'file603.txt': [40], 'file913.txt': [55], 'file904.txt': [19], 'file223.txt': [9], 'file448.txt': [12], 'file126.txt': [68], 'file571.txt': [41], 'file784.txt': [24], 'file953.txt': [66], 'file414.txt': [61], 'file984.txt': [5], 'file937.txt': [32, 75, 86], 'file33.txt': [46], 'file760.txt': [12], 'file55.txt': [16], 'file65.txt': [55, 90], 'file425.txt': [48], 'file4.txt': [4], 'file239.txt': [26], 'file575.txt': [30], 'file284.txt': [4], 'file200.txt': [21], 'file379.txt': [0], 'file660.txt': [1, 40], 'file798.txt': [27], 'file738.txt': [9], 'file235.txt': [30], 'file413.txt': [82], 'file710.txt': [6], 'file6.txt': [20], 'file422.txt': [36], 'file952.txt': [3], 'file545.txt': [22], 'file957.txt': [1, 10], 'file117.txt': [13], 'file172.txt': [72], 'file328.txt': [36], 'file439.txt': [42], 'file107.txt': [69], 'file912.txt': [34, 57, 73], 'file609.txt': [0], 'file720.txt': [18], 'file222.txt': [62], 'file139.txt': [10, 13], 'file864.txt': [25], 'file920.txt': [8, 15], 'file654.txt': [38], 'file627.txt': [76], 'file232.txt': [19], 'file509.txt': [3], 'file396.txt': [76], 'file806.txt': [16], 'file243.txt': [10], 'file532.txt': [18], 'file719.txt': [60], 'file159.txt': [68], 'file359.txt': [0, 21], 'file49.txt': [25, 31], 'file951.txt': [1, 78], 'file588.txt': [16, 34], 'file362.txt': [35, 43], 'file529.txt': [22], 'file821.txt': [15, 28], 'file447.txt': [3], 'file836.txt': [7], 'file702.txt': [22], 'file77.txt': [34], 'file737.txt': [26], 'file311.txt': [24], 'file689.txt': [15], 'file618.txt': [40], 'file326.txt': [0], 'file714.txt': [5, 19], 'file436.txt': [27], 'file334.txt': [18, 48, 50], 'file303.txt': [34], 'file479.txt': [13], 'file66.txt': [10], 'file156.txt': [77], 'file312.txt': [8, 31, 50, 60], 'file440.txt': [36], 'file899.txt': [37], 'file885.txt': [27], 'file669.txt': [4], 'file524.txt': [54, 85], 'file324.txt': [74, 91], 'file962.txt': [17], 'file28.txt': [6, 77], 'file764.txt': [21], 'file879.txt': [7, 16, 40], 'file805.txt': [35, 64], 'file127.txt': [15], 'file911.txt': [53, 59], 'file267.txt': [54, 78], 'file68.txt': [18], 'file578.txt': [5, 63, 74, 77, 94], 'file370.txt': [1], 'file787.txt': [19, 20, 24], 'file415.txt': [16], 'file208.txt': [9], 'file613.txt': [4], 'file385.txt': [3], 'file898.txt': [47], 'file694.txt': [15], 'file907.txt': [11, 35, 38], 'file349.txt': [31], 'file746.txt': [49], 'file132.txt': [3], 'file43.txt': [7], 'file681.txt': [39, 43, 47], 'file120.txt': [2], 'file279.txt': [69, 82], 'file596.txt': [11, 16, 22], 'file964.txt': [12, 25], 'file944.txt': [12], 'file640.txt': [11], 'file360.txt': [3], 'file443.txt': [48], 'file556.txt': [28], 'file371.txt': [14], 'file358.txt': [8], 'file993.txt': [21], 'file307.txt': [18], 'file863.txt': [7], 'file550.txt': [68], 'file514.txt': [5, 51], 'file103.txt': [39], 'file277.txt': [30], 'file906.txt': [49, 54], 'file322.txt': [89], 'file986.txt': [23, 42], 'file558.txt': [25], 'file511.txt': [7], 'file138.txt': [37], 'file290.txt': [8], 'file659.txt': [19, 28], 'file353.txt': [6], 'file703.txt': [144, 187, 269], 'file48.txt': [24, 36, 60, 78], 'file520.txt': [39], 'file461.txt': [50], 'file833.txt': [29], 'file152.txt': [2, 26, 35], 'file794.txt': [18], 'file378.txt': [15], 'file637.txt': [14, 73], 'file716.txt': [0, 11, 27, 50], 'file30.txt': [10], 'file241.txt': [26], 'file462.txt': [18, 27], 'file852.txt': [13], 'file59.txt': [39], 'file731.txt': [9, 27], 'file870.txt': [23, 57], 'file853.txt': [24], 'file213.txt': [5], 'file777.txt': [33], 'file46.txt': [33], 'file404.txt': [29, 61, 86], 'file534.txt': [64], 'file980.txt': [41, 48], 'file801.txt': [36, 86], 'file293.txt': [11, 31], 'file338.txt': [50], 'file194.txt': [42, 64, 67], 'file292.txt': [67], 'file701.txt': [2, 7, 47, 53], 'file45.txt': [27, 42, 61], 'file781.txt': [60, 74], 'file896.txt': [9], 'file438.txt': [13, 44, 51], 'file501.txt': [12], 'file602.txt': [4], 'file373.txt': [50], 'file231.txt': [0], 'file915.txt': [35, 66], 'file632.txt': [16, 40, 47], 'file935.txt': [18, 34], 'file256.txt': [38], 'file700.txt': [31, 33], 'file839.txt': [30], 'file968.txt': [22], 'file876.txt': [10], 'file910.txt': [58], 'file246.txt': [19], 'file766.txt': [17], 'file481.txt': [2], 'file467.txt': [83], 'file782.txt': [3, 9], 'file118.txt': [48], 'file269.txt': [66], 'file564.txt': [32], 'file289.txt': [13], 'file201.txt': [35], 'file796.txt': [61], 'file464.txt': [29], 'file146.txt': [13], 'file3.txt': [12, 19], 'file684.txt': [50], 'file143.txt': [47], 'file457.txt': [15], 'file553.txt': [28], 'file61.txt': [1, 12, 34, 41], 'file214.txt': [57], 'file111.txt': [17], 'file169.txt': [16], 'file463.txt': [10], 'file332.txt': [9], 'file615.txt': [2], 'file141.txt': [43], 'file212.txt': [0], 'file383.txt': [47], 'file228.txt': [6, 99], 'file434.txt': [9], 'file838.txt': [9], 'file543.txt': [38], 'file868.txt': [16, 26], 'file186.txt': [7], 'file504.txt': [5, 9], 'file608.txt': [77], 'file827.txt': [8], 'file78.txt': [1, 3, 11, 23], 'file978.txt': [15], 'file423.txt': [36], 'file886.txt': [32, 59], 'file848.txt': [27, 90], 'file301.txt': [3], 'file628.txt': [13], 'file680.txt': [19, 26], 'file258.txt': [2, 8], 'file309.txt': [24], 'file42.txt': [60], 'file306.txt': [23], 'file320.txt': [33], 'file39.txt': [37], 'file563.txt': [7, 57], 'file112.txt': [3], 'file507.txt': [51], 'file101.txt': [38], 'file830.txt': [34], 'file491.txt': [14, 44], 'file79.txt': [38, 86], 'file221.txt': [4], 'file947.txt': [20], 'file238.txt': [21], 'file650.txt': [20], 'file148.txt': [39], 'file862.txt': [26], 'file429.txt': [13], 'file584.txt': [10], 'file294.txt': [90], 'file325.txt': [64, 81, 87, 89], 'file961.txt': [12, 53], 'file815.txt': [12, 29], 'file539.txt': [23, 44, 60], 'file19.txt': [25, 38], 'file835.txt': [4, 13], 'file245.txt': [27, 30, 55], 'file363.txt': [10, 23, 64], 'file233.txt': [38], 'file17.txt': [25, 37], 'file329.txt': [34, 46], 'file166.txt': [71], 'file930.txt': [20], 'file424.txt': [0], 'file566.txt': [16], 'file388.txt': [20], 'file528.txt': [0], 'file630.txt': [10], 'file129.txt': [37, 47, 86], 'file428.txt': [30, 36, 95], 'file675.txt': [23], 'file744.txt': [16, 48, 61], 'file880.txt': [21, 106], 'file134.txt': [12], 'file342.txt': [38, 50], 'file926.txt': [3], 'file69.txt': [18, 20, 33], 'file665.txt': [85], 'file546.txt': [20], 'file56.txt': [18], 'file526.txt': [7], 'file538.txt': [18], 'file793.txt': [29], 'file455.txt': [26, 31, 40], 'file995.txt': [12, 40], 'file2.txt': [19], 'file281.txt': [20], 'file47.txt': [11, 23, 48, 57], 'file227.txt': [5], 'file453.txt': [86], 'file175.txt': [10], 'file179.txt': [15, 18], 'file430.txt': [53], 'file595.txt': [2], 'file767.txt': [20], 'file211.txt': [5], 'file812.txt': [25, 29], 'file81.txt': [26], 'file22.txt': [45], 'file918.txt': [49], 'file410.txt': [8, 40, 48], 'file890.txt': [76], 'file122.txt': [16, 24, 33], 'file249_preprocessed.txt': [64], 'file674_preprocessed.txt': [60], 'file249.txt': [64], 'file14.txt': [12], 'file761.txt': [9]}\n",
            "Term 'even' appears in documents and positions: {'file728.txt': [25], 'file405.txt': [32], 'file542.txt': [11], 'file253.txt': [10], 'file554.txt': [18, 21], 'file372.txt': [35, 38, 57], 'file52.txt': [6], 'file808.txt': [44], 'file638.txt': [76], 'file240.txt': [62], 'file603.txt': [2], 'file593.txt': [15], 'file992.txt': [20], 'file65.txt': [35, 84], 'file397.txt': [6], 'file460.txt': [11, 23], 'file422.txt': [14], 'file867.txt': [31], 'file328.txt': [27], 'file875.txt': [30], 'file912.txt': [10], 'file468.txt': [35], 'file720.txt': [49], 'file928.txt': [52], 'file809.txt': [7, 15], 'file920.txt': [34, 39], 'file248.txt': [14, 82], 'file49.txt': [6, 12], 'file951.txt': [37], 'file702.txt': [44], 'file326.txt': [74], 'file844.txt': [11], 'file465.txt': [19], 'file440.txt': [13], 'file524.txt': [21], 'file106.txt': [60], 'file998.txt': [12], 'file449.txt': [15], 'file573.txt': [86], 'file578.txt': [14], 'file437.txt': [7], 'file898.txt': [14], 'file160.txt': [30], 'file907.txt': [17], 'file746.txt': [39], 'file681.txt': [40], 'file661.txt': [10], 'file987.txt': [5], 'file102.txt': [61], 'file360.txt': [56], 'file993.txt': [9], 'file863.txt': [26], 'file277.txt': [14], 'file906.txt': [23], 'file138.txt': [28, 61], 'file48.txt': [71], 'file716.txt': [48], 'file404.txt': [30], 'file980.txt': [38], 'file293.txt': [10], 'file292.txt': [33], 'file701.txt': [17], 'file611.txt': [13], 'file896.txt': [25], 'file438.txt': [14], 'file501.txt': [13], 'file256.txt': [18], 'file20.txt': [26], 'file989.txt': [35], 'file934.txt': [15, 39], 'file873.txt': [35], 'file553.txt': [69], 'file871.txt': [30], 'file332.txt': [30], 'file141.txt': [10], 'file228.txt': [70], 'file819.txt': [14], 'file116.txt': [8], 'file978.txt': [8], 'file848.txt': [60], 'file914.txt': [5], 'file150.txt': [40], 'file42.txt': [42], 'file834.txt': [18], 'file745.txt': [27], 'file748.txt': [107], 'file112.txt': [31], 'file507.txt': [5], 'file491.txt': [17], 'file79.txt': [11, 26], 'file100.txt': [51], 'file862.txt': [30], 'file835.txt': [47], 'file245.txt': [38], 'file679.txt': [72], 'file859.txt': [26], 'file166.txt': [29], 'file930.txt': [13], 'file428.txt': [12], 'file305.txt': [56], 'file544.txt': [8], 'file286.txt': [40], 'file453.txt': [73], 'file295.txt': [17], 'file430.txt': [32], 'file773.txt': [14], 'file776.txt': [8], 'file206.txt': [7], 'file390.txt': [61], 'file125.txt': [18]}\n",
            "Term 'punched' appears in documents and positions: {'file728.txt': [26]}\n",
            "Term 'main' appears in documents and positions: {'file728.txt': [27], 'file847.txt': [47], 'file240.txt': [6], 'file366.txt': [60], 'file435.txt': [29], 'file654.txt': [29], 'file324.txt': [76], 'file578.txt': [31], 'file787.txt': [58], 'file878.txt': [14], 'file883.txt': [5], 'file225.txt': [10]}\n",
            "Term 'reason' appears in documents and positions: {'file728.txt': [28, 63], 'file25.txt': [8], 'file813.txt': [33], 'file119.txt': [3], 'file784.txt': [18], 'file576.txt': [40], 'file385.txt': [2], 'file871.txt': [61], 'file748.txt': [116], 'file295.txt': [4], 'file767.txt': [19], 'file225.txt': [11]}\n",
            "Term 'bought' appears in documents and positions: {'file728.txt': [29], 'file131.txt': [6], 'file708.txt': [77], 'file119.txt': [1], 'file771.txt': [2], 'file808.txt': [5], 'file760.txt': [2], 'file533.txt': [3], 'file490.txt': [64], 'file562.txt': [3], 'file65.txt': [0], 'file750.txt': [3], 'file284.txt': [1], 'file407.txt': [49], 'file842.txt': [0], 'file5.txt': [0], 'file107.txt': [1], 'file411.txt': [0], 'file912.txt': [2], 'file720.txt': [4], 'file139.txt': [17], 'file654.txt': [13], 'file232.txt': [0], 'file176.txt': [2, 35, 38], 'file248.txt': [0], 'file588.txt': [55], 'file311.txt': [16], 'file183.txt': [8], 'file714.txt': [0], 'file9.txt': [0], 'file35.txt': [7, 57], 'file80.txt': [0], 'file669.txt': [29], 'file524.txt': [41], 'file324.txt': [16], 'file267.txt': [11], 'file790.txt': [56], 'file559.txt': [13, 95], 'file360.txt': [0], 'file556.txt': [0], 'file431.txt': [33], 'file157.txt': [70], 'file322.txt': [46], 'file986.txt': [0], 'file40.txt': [8], 'file483.txt': [7], 'file743.txt': [17, 19], 'file624.txt': [2], 'file607.txt': [0, 6], 'file30.txt': [3], 'file63.txt': [0], 'file803.txt': [9], 'file741.txt': [4], 'file895.txt': [0], 'file45.txt': [0], 'file632.txt': [0], 'file551.txt': [9], 'file380.txt': [27], 'file467.txt': [29, 88], 'file989.txt': [17], 'file269.txt': [26], 'file244.txt': [10, 41], 'file457.txt': [0], 'file214.txt': [16], 'file878.txt': [0, 18], 'file332.txt': [13], 'file966.txt': [5], 'file838.txt': [35], 'file186.txt': [0], 'file400.txt': [6], 'file38.txt': [0], 'file886.txt': [26], 'file905.txt': [16], 'file699.txt': [0], 'file484.txt': [0], 'file491.txt': [61], 'file770.txt': [85], 'file676.txt': [5], 'file392.txt': [1], 'file325.txt': [0], 'file961.txt': [0], 'file815.txt': [0], 'file19.txt': [0], 'file547.txt': [0], 'file363.txt': [0], 'file163.txt': [6], 'file566.txt': [0], 'file64.txt': [88], 'file154.txt': [32], 'file675.txt': [2], 'file744.txt': [57], 'file538.txt': [0], 'file81.txt': [19], 'file265.txt': [14]}\n",
            "Term 'store' appears in documents and positions: {'file728.txt': [30], 'file65.txt': [20], 'file916.txt': [48], 'file216.txt': [3], 'file928.txt': [11], 'file719.txt': [49], 'file248.txt': [5], 'file955.txt': [20, 27], 'file647.txt': [2], 'file287.txt': [9], 'file637.txt': [71, 83], 'file45.txt': [38], 'file110.txt': [8], 'file766.txt': [23], 'file684.txt': [80], 'file880.txt': [86], 'file665.txt': [30]}\n",
            "Term 'noisy' appears in documents and positions: {'file728.txt': [31], 'file760.txt': [70], 'file739.txt': [24], 'file687.txt': [6], 'file927.txt': [12]}\n",
            "Term 'external' appears in documents and positions: {'file728.txt': [32], 'file708.txt': [34], 'file182.txt': [14], 'file703.txt': [6, 25, 37, 46, 76], 'file192.txt': [10], 'file388.txt': [15], 'file331.txt': [26]}\n",
            "Term 'hard' appears in documents and positions: {'file728.txt': [33], 'file542.txt': [7], 'file735.txt': [6], 'file888.txt': [14], 'file374.txt': [4], 'file73.txt': [74], 'file953.txt': [46], 'file758.txt': [37], 'file105.txt': [89], 'file660.txt': [37], 'file235.txt': [34], 'file280.txt': [17], 'file474.txt': [3], 'file752.txt': [20], 'file362.txt': [24], 'file311.txt': [2, 8, 18], 'file979.txt': [47], 'file28.txt': [55, 93], 'file531.txt': [7], 'file255.txt': [2], 'file907.txt': [53], 'file746.txt': [2, 13, 16, 35], 'file559.txt': [88], 'file993.txt': [2], 'file726.txt': [11], 'file517.txt': [12], 'file733.txt': [40], 'file534.txt': [19], 'file935.txt': [28, 72], 'file692.txt': [9], 'file79.txt': [65, 84], 'file148.txt': [33], 'file387.txt': [7], 'file858.txt': [10, 68], 'file331.txt': [15], 'file108.txt': [38], 'file376.txt': [6], 'file22.txt': [1], 'file265.txt': [8]}\n",
            "Term 'drive' appears in documents and positions: {'file728.txt': [34], 'file441.txt': [27], 'file336.txt': [31, 37, 93], 'file979.txt': [10], 'file323.txt': [1], 'file45.txt': [35], 'file968.txt': [5], 'file978.txt': [28]}\n",
            "Term 'ca' appears in documents and positions: {'file728.txt': [35], 'file982.txt': [20], 'file709.txt': [26], 'file736.txt': [1], 'file389.txt': [15], 'file847.txt': [55], 'file343.txt': [66, 97], 'file414.txt': [60], 'file937.txt': [85], 'file660.txt': [0], 'file6.txt': [19], 'file957.txt': [0], 'file912.txt': [33, 72], 'file920.txt': [7], 'file627.txt': [75], 'file243.txt': [9], 'file49.txt': [24], 'file951.txt': [0], 'file312.txt': [49], 'file324.txt': [90], 'file805.txt': [34], 'file911.txt': [58], 'file578.txt': [73], 'file370.txt': [0], 'file613.txt': [3], 'file746.txt': [48], 'file322.txt': [88], 'file520.txt': [38], 'file637.txt': [13], 'file534.txt': [63], 'file781.txt': [59], 'file438.txt': [50], 'file632.txt': [15], 'file968.txt': [21], 'file464.txt': [28], 'file214.txt': [56], 'file332.txt': [8], 'file423.txt': [35], 'file848.txt': [89], 'file491.txt': [13], 'file238.txt': [20], 'file325.txt': [86], 'file539.txt': [59], 'file134.txt': [11], 'file69.txt': [32], 'file546.txt': [19], 'file526.txt': [6], 'file47.txt': [22], 'file453.txt': [85], 'file211.txt': [4], 'file22.txt': [44], 'file410.txt': [47], 'file890.txt': [75]}\n",
            "Term 'guess' appears in documents and positions: {'file728.txt': [37], 'file237.txt': [7], 'file654.txt': [47], 'file243.txt': [7], 'file805.txt': [70], 'file640.txt': [14], 'file659.txt': [2, 16], 'file700.txt': [27], 'file363.txt': [71], 'file495.txt': [7], 'file880.txt': [104]}\n",
            "Term 'drawer' appears in documents and positions: {'file728.txt': [38, 52], 'file735.txt': [3]}\n",
            "Term 'going' appears in documents and positions: {'file728.txt': [39], 'file841.txt': [48], 'file224.txt': [13], 'file420.txt': [19], 'file44.txt': [23], 'file784.txt': [20], 'file660.txt': [16], 'file460.txt': [61], 'file435.txt': [43], 'file124.txt': [10], 'file920.txt': [41], 'file579.txt': [26], 'file509.txt': [20], 'file396.txt': [90], 'file159.txt': [26], 'file983.txt': [29], 'file837.txt': [5], 'file885.txt': [57], 'file911.txt': [28], 'file573.txt': [61, 96], 'file255.txt': [30], 'file198.txt': [8], 'file647.txt': [34], 'file483.txt': [55], 'file637.txt': [34], 'file177.txt': [15], 'file741.txt': [31], 'file194.txt': [32], 'file133.txt': [26], 'file831.txt': [47], 'file684.txt': [88], 'file884.txt': [19], 'file90.txt': [16], 'file214.txt': [45], 'file600.txt': [12], 'file849.txt': [65], 'file309.txt': [11], 'file634.txt': [1], 'file691.txt': [28, 50], 'file497.txt': [27], 'file325.txt': [57], 'file835.txt': [43], 'file329.txt': [8], 'file825.txt': [48]}\n",
            "Term 'hold' appears in documents and positions: {'file728.txt': [40], 'file542.txt': [63], 'file223.txt': [11], 'file283.txt': [19], 'file490.txt': [11], 'file200.txt': [11], 'file738.txt': [19], 'file818.txt': [0], 'file720.txt': [55], 'file861.txt': [42], 'file308.txt': [22, 28], 'file689.txt': [37], 'file618.txt': [2], 'file326.txt': [21], 'file436.txt': [29], 'file303.txt': [15], 'file789.txt': [7], 'file974.txt': [18], 'file84.txt': [90], 'file536.txt': [4], 'file805.txt': [40], 'file127.txt': [22], 'file267.txt': [30, 37, 64, 69], 'file279.txt': [58], 'file358.txt': [5], 'file993.txt': [27], 'file99.txt': [40], 'file655.txt': [8], 'file749.txt': [33], 'file703.txt': [173], 'file716.txt': [30], 'file462.txt': [22], 'file777.txt': [9], 'file632.txt': [41], 'file616.txt': [37], 'file564.txt': [69], 'file121.txt': [5], 'file868.txt': [27], 'file186.txt': [17], 'file886.txt': [60], 'file518.txt': [36], 'file499.txt': [0], 'file914.txt': [2], 'file584.txt': [47], 'file539.txt': [45], 'file865.txt': [50], 'file64.txt': [21], 'file189.txt': [10], 'file125.txt': [15], 'file265.txt': [59], 'file14.txt': [16, 26]}\n",
            "Term 'random' appears in documents and positions: {'file728.txt': [41]}\n",
            "Term 'crap' appears in documents and positions: {'file728.txt': [42], 'file365.txt': [2], 'file879.txt': [18], 'file338.txt': [48], 'file910.txt': [44], 'file544.txt': [22]}\n",
            "Term 'desk' appears in documents and positions: {'file728.txt': [43], 'file738.txt': [0], 'file84.txt': [27, 63, 91], 'file267.txt': [86], 'file578.txt': [45], 'file958.txt': [4], 'file146.txt': [4, 26]}\n",
            "Term ''m' appears in documents and positions: {'file728.txt': [44], 'file735.txt': [8], 'file372.txt': [0], 'file742.txt': [18, 20], 'file343.txt': [30], 'file945.txt': [3], 'file347.txt': [61], 'file942.txt': [3], 'file223.txt': [12], 'file412.txt': [39], 'file784.txt': [19], 'file195.txt': [22], 'file992.txt': [52], 'file414.txt': [2, 48], 'file760.txt': [47], 'file649.txt': [29], 'file533.txt': [17], 'file55.txt': [21, 43], 'file366.txt': [1], 'file946.txt': [2, 9], 'file239.txt': [52], 'file788.txt': [24, 27, 40], 'file235.txt': [42], 'file144.txt': [20], 'file168.txt': [48], 'file912.txt': [26], 'file124.txt': [1], 'file654.txt': [43], 'file678.txt': [17], 'file232.txt': [28], 'file396.txt': [7, 89], 'file983.txt': [31], 'file702.txt': [25], 'file780.txt': [5], 'file936.txt': [62], 'file288.txt': [12], 'file183.txt': [0, 30, 43], 'file714.txt': [11, 25], 'file465.txt': [3], 'file440.txt': [5], 'file314.txt': [66, 87], 'file885.txt': [4, 8, 11], 'file962.txt': [41], 'file879.txt': [13], 'file667.txt': [15], 'file805.txt': [7], 'file911.txt': [27], 'file318.txt': [2], 'file578.txt': [87], 'file349.txt': [36], 'file559.txt': [93], 'file640.txt': [17, 23, 32, 36, 50], 'file408.txt': [16], 'file990.txt': [17], 'file550.txt': [74], 'file514.txt': [34], 'file791.txt': [7], 'file749.txt': [15], 'file137.txt': [15], 'file353.txt': [32], 'file624.txt': [21], 'file731.txt': [12], 'file404.txt': [44, 78], 'file398.txt': [2], 'file16.txt': [4], 'file45.txt': [66], 'file611.txt': [6], 'file781.txt': [64, 69], 'file896.txt': [5], 'file501.txt': [17], 'file231.txt': [8], 'file632.txt': [44], 'file256.txt': [7, 24], 'file831.txt': [46], 'file380.txt': [17, 59], 'file467.txt': [37, 66], 'file564.txt': [19, 46], 'file340.txt': [26], 'file469.txt': [15, 21, 24], 'file684.txt': [83, 87], 'file244.txt': [39], 'file111.txt': [4], 'file687.txt': [34, 38, 44], 'file332.txt': [0], 'file212.txt': [18], 'file966.txt': [0], 'file434.txt': [38], 'file186.txt': [15], 'file686.txt': [36], 'file519.txt': [37], 'file978.txt': [43], 'file423.txt': [24], 'file516.txt': [0], 'file722.txt': [24], 'file391.txt': [1], 'file680.txt': [17], 'file29.txt': [59], 'file634.txt': [0, 26], 'file42.txt': [24, 28], 'file769.txt': [45], 'file691.txt': [6, 26], 'file810.txt': [0, 22], 'file830.txt': [6], 'file234.txt': [37], 'file597.txt': [20], 'file491.txt': [0], 'file497.txt': [26], 'file148.txt': [66], 'file406.txt': [36], 'file294.txt': [75], 'file325.txt': [56, 76], 'file193.txt': [0], 'file815.txt': [25], 'file539.txt': [0], 'file24.txt': [37], 'file547.txt': [5, 43], 'file865.txt': [14], 'file673.txt': [16], 'file923.txt': [43], 'file375.txt': [35, 45], 'file387.txt': [42], 'file129.txt': [42], 'file444.txt': [11], 'file51.txt': [60], 'file793.txt': [48], 'file527.txt': [19], 'file772.txt': [23], 'file890.txt': [3], 'file249_preprocessed.txt': [29], 'file249.txt': [29]}\n",
            "Term 'honestly' appears in documents and positions: {'file728.txt': [45], 'file343.txt': [42], 'file787.txt': [23], 'file340.txt': [27], 'file519.txt': [2], 'file827.txt': [7], 'file491.txt': [68], 'file363.txt': [63], 'file163.txt': [56]}\n",
            "Term 'bummed' appears in documents and positions: {'file728.txt': [47]}\n",
            "Term 'tempted' appears in documents and positions: {'file728.txt': [48], 'file510.txt': [2]}\n",
            "Term 'give' appears in documents and positions: {'file728.txt': [49], 'file982.txt': [12], 'file817.txt': [13], 'file253.txt': [39], 'file991.txt': [35], 'file784.txt': [21], 'file200.txt': [13], 'file422.txt': [13], 'file172.txt': [23], 'file216.txt': [5], 'file936.txt': [39], 'file151.txt': [7], 'file885.txt': [52], 'file324.txt': [3], 'file263.txt': [2], 'file385.txt': [4], 'file43.txt': [15], 'file279.txt': [32], 'file360.txt': [52], 'file846.txt': [38], 'file863.txt': [23], 'file191.txt': [21], 'file48.txt': [82], 'file637.txt': [85], 'file980.txt': [20], 'file525.txt': [39], 'file217.txt': [3], 'file839.txt': [47], 'file340.txt': [34], 'file684.txt': [89], 'file155.txt': [9], 'file819.txt': [71], 'file400.txt': [44], 'file634.txt': [2], 'file36.txt': [36], 'file54.txt': [39], 'file691.txt': [35], 'file770.txt': [75], 'file539.txt': [11, 19], 'file363.txt': [20], 'file163.txt': [0], 'file880.txt': [9], 'file908.txt': [12], 'file295.txt': [1], 'file767.txt': [44, 52], 'file825.txt': [1], 'file22.txt': [2], 'file918.txt': [52]}\n",
            "Term 'two' appears in documents and positions: {'file728.txt': [50], 'file542.txt': [14, 16], 'file224.txt': [21], 'file813.txt': [81], 'file742.txt': [55], 'file347.txt': [49], 'file260.txt': [16], 'file448.txt': [14], 'file264.txt': [6], 'file393.txt': [8], 'file490.txt': [23], 'file105.txt': [56], 'file65.txt': [7], 'file200.txt': [7], 'file115.txt': [44], 'file407.txt': [37], 'file348.txt': [8], 'file107.txt': [104], 'file627.txt': [90], 'file617.txt': [1], 'file268.txt': [14], 'file49.txt': [20], 'file203.txt': [15], 'file326.txt': [16], 'file690.txt': [45], 'file828.txt': [41], 'file712.txt': [43], 'file66.txt': [2], 'file323.txt': [20], 'file314.txt': [18, 69], 'file911.txt': [5], 'file106.txt': [44], 'file276.txt': [0], 'file578.txt': [11], 'file787.txt': [37], 'file790.txt': [58], 'file746.txt': [42], 'file31.txt': [27], 'file443.txt': [38], 'file157.txt': [1], 'file517.txt': [32, 42], 'file322.txt': [53], 'file558.txt': [77], 'file743.txt': [7, 20], 'file703.txt': [175], 'file63.txt': [16], 'file37.txt': [13], 'file271.txt': [7], 'file577.txt': [6], 'file45.txt': [1, 6], 'file915.txt': [71], 'file700.txt': [16, 19], 'file831.txt': [49], 'file989.txt': [18], 'file684.txt': [10, 33, 38, 41], 'file553.txt': [46], 'file214.txt': [43], 'file332.txt': [14], 'file725.txt': [6], 'file186.txt': [1], 'file309.txt': [17], 'file494.txt': [8], 'file834.txt': [28], 'file54.txt': [27], 'file204.txt': [18], 'file770.txt': [88], 'file100.txt': [17], 'file639.txt': [27], 'file835.txt': [49], 'file859.txt': [4], 'file17.txt': [3], 'file424.txt': [10], 'file375.txt': [13], 'file305.txt': [65, 72], 'file51.txt': [1, 14], 'file744.txt': [21], 'file665.txt': [81], 'file772.txt': [4], 'file108.txt': [10], 'file949_preprocessed.txt': [87, 88], 'file949.txt': [87, 88]}\n",
            "Term 'stars' appears in documents and positions: {'file728.txt': [51], 'file813.txt': [68], 'file275.txt': [1], 'file240.txt': [66], 'file913.txt': [94], 'file200.txt': [15], 'file545.txt': [21], 'file168.txt': [51], 'file314.txt': [90], 'file576.txt': [43], 'file385.txt': [6], 'file987.txt': [13], 'file99.txt': [16], 'file525.txt': [41], 'file781.txt': [73], 'file935.txt': [78, 97], 'file700.txt': [17], 'file634.txt': [4], 'file36.txt': [38], 'file584.txt': [54], 'file815.txt': [65], 'file679.txt': [52], 'file363.txt': [22], 'file163.txt': [3], 'file923.txt': [41], 'file295.txt': [3], 'file767.txt': [23, 54], 'file108.txt': [93], 'file225.txt': [14], 'file22.txt': [5], 'file918.txt': [54]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pickle\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    return words\n",
        "\n",
        "def find_documents_for_query(query, positional_index):\n",
        "    query_terms = preprocess_text(query)\n",
        "    documents = None\n",
        "    for i, term in enumerate(query_terms):\n",
        "        if term in positional_index:\n",
        "            if documents is None:\n",
        "                documents = {doc: positions for doc, positions in positional_index[term].items()}\n",
        "            else:\n",
        "                temp_documents = {}\n",
        "                for doc, positions in documents.items():\n",
        "                    if doc in positional_index[term]:\n",
        "                        new_positions = [pos for pos in positional_index[term][doc] if pos-1 in positions]\n",
        "                        if new_positions:\n",
        "                            temp_documents[doc] = new_positions\n",
        "                documents = temp_documents\n",
        "        else:\n",
        "            return set()\n",
        "    return set(documents.keys())\n",
        "\n",
        "def user_input_and_search(loaded_positional_index):\n",
        "    n = int(input(\"Enter number of queries: \"))\n",
        "    for i in range(n):\n",
        "        query = input(f\"Enter query {i+1}: \")\n",
        "        matching_docs = find_documents_for_query(query, loaded_positional_index)\n",
        "        print(f\"Number of documents retrieved for query {i+1} using positional index: {len(matching_docs)}\")\n",
        "        if matching_docs:\n",
        "            print(f\"Names of documents retrieved for query {i+1} using positional index: {', '.join(sorted(matching_docs))}\")\n",
        "        else:\n",
        "            print(\"No documents found.\")\n",
        "\n",
        "\n",
        "\n",
        "user_input_and_search(loaded_positional_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOWgl-d9ZIw1",
        "outputId": "046dadad-3cf2-41a5-a0a6-28d040f03751"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of queries: 1\n",
            "Enter query 1: product\n",
            "Number of documents retrieved for query 1 using positional index: 106\n",
            "Names of documents retrieved for query 1 using positional index: file10.txt, file105.txt, file115.txt, file121.txt, file134.txt, file142.txt, file152.txt, file167.txt, file168.txt, file171.txt, file178.txt, file180.txt, file186.txt, file19.txt, file196.txt, file208.txt, file217.txt, file225.txt, file228.txt, file229.txt, file241.txt, file251.txt, file26.txt, file269.txt, file282.txt, file298.txt, file30.txt, file326.txt, file337.txt, file35.txt, file358.txt, file363.txt, file372.txt, file374.txt, file375.txt, file378.txt, file388.txt, file389.txt, file404.txt, file42.txt, file422.txt, file423.txt, file425.txt, file435.txt, file437.txt, file443.txt, file447.txt, file46.txt, file460.txt, file477.txt, file487.txt, file501.txt, file525.txt, file544.txt, file558.txt, file561.txt, file565.txt, file569.txt, file573.txt, file576.txt, file586.txt, file592.txt, file616.txt, file622.txt, file634.txt, file64.txt, file647.txt, file659.txt, file677.txt, file68.txt, file685.txt, file698.txt, file70.txt, file718.txt, file719.txt, file720.txt, file75.txt, file758.txt, file769.txt, file783.txt, file793.txt, file815.txt, file817.txt, file819.txt, file831.txt, file835.txt, file843.txt, file849.txt, file861.txt, file872.txt, file888.txt, file898.txt, file917.txt, file923.txt, file931.txt, file935.txt, file938.txt, file943.txt, file944.txt, file945.txt, file947.txt, file951.txt, file956.txt, file96.txt, file966.txt, file999.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PcGZfUcxeiKK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}