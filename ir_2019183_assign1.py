# -*- coding: utf-8 -*-
"""IR_2019183_Assign1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MpxDh0et7-BaU_h6GgiBI1g_BsFYgkGX
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install nltk
import nltk
nltk.download('punkt')
nltk.download('stopwords')

"""# **Q1. Data Preprocessing**

*   List item
*   List item


"""

import os
import random
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string
import re

import nltk
nltk.download('punkt')
nltk.download('stopwords')


# Path to the dataset directory
dataset_dir = '/content/drive/MyDrive/IR_DataSet_Files/text_files-20240206T164431Z-001/text_files'


all_files = os.listdir(dataset_dir)


random_files = random.sample(all_files, 5)

for filename in random_files:
    file_path = os.path.join(dataset_dir, filename)
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()

        # Printing  original content
        print(f"Original content of {filename}:\n{content}\n")

        # Lowercase the text of the original content
        lowercased_content = content.lower()

        # Print lowercased content after the lowering as a step1.
        print(f"Lowercased content of {filename}:\n{lowercased_content}\n")

"""**Perform tokenization**"""

# Set of English stopwords
stop_words = set(stopwords.words('english'))

# Iterate over each of the 5 randomly selected files
for filename in random_files:
    file_path = os.path.join(dataset_dir, filename)
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()

        # Print original content
        print(f"Original content of {filename}:\n{content}\n")

        # Lowercase the text
        lowercased_content = content.lower()

        # Tokenize the lowercased content
        tokens = word_tokenize(lowercased_content)

        # Print tokenized content after the process.
        print(f"Tokenized content of {filename}:\n{tokens}\n")

        # Removing  stopwords
        tokens_without_stopwords = [word for word in tokens if word not in stop_words]

        # Print the content after removing stopwords
        print(f"Content of {filename} after removing stopwords:\n{' '.join(tokens_without_stopwords)}\n")


        # Remove punctuations and blank space tokens
        tokens_without_punctuations_and_spaces = [word for word in tokens_without_stopwords if word not in string.punctuation and word.strip()]

        # Print the content after removing punctuations and blank spaces
        print(f"Content of {filename} after removing punctuations and blank spaces:\n{' '.join(tokens_without_punctuations_and_spaces)}\n")

# Define your source and target directories
source_dir = '/content/drive/MyDrive/IR_DataSet_Files/text_files-20240206T164431Z-001/text_files'
target_dir = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessed_Files'

# Create the target directory if it doesn't exist
if not os.path.exists(target_dir):
    os.makedirs(target_dir)

def preprocess_text(text):
    # Lowercase the text
    text = text.lower()
    # Tokenize the text
    tokens = word_tokenize(text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    # Remove punctuations and blank space tokens
    tokens = [word for word in tokens if word not in string.punctuation and word.strip()]
    # Rejoin tokens into a string
    preprocessed_text = ' '.join(tokens)
    return preprocessed_text

# Define a function to preprocess the text
def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove all punctuation from the text, including apostrophes, but preserve intra-word apostrophes
    text = re.sub(r'\b\'|\'\b', '', text)  # Remove leading and trailing apostrophes
    text = re.sub(r'(?<!\w)[^\s\w]+|[^\s\w]+(?!\w)', '', text)  # Remove punctuation without removing intra-word apostrophes

    # Tokenize the text
    tokens = word_tokenize(text)

    # Filter out stop words
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]

    return filtered_tokens

# Example usage
original_text = "it's more on the toy side than on the instrument side, and made in Indonesia."
preprocessed_tokens = preprocess_text(original_text)
print("Preprocessed text:", ' '.join(preprocessed_tokens))

# for filename in os.listdir(source_dir):
#     file_path = os.path.join(source_dir, filename)
#     # Ensure we're only working with files (and, for example, not directories)
#     if os.path.isfile(file_path):
#         with open(file_path, 'r', encoding='utf-8') as file:
#             content = file.read()
#             preprocessed_content = preprocess_text(content)
#             # Define the path for the new file in the target directory
#             new_file_path = os.path.join(target_dir, filename)
#             with open(new_file_path, 'w', encoding='utf-8') as new_file:
#                 new_file.write(preprocessed_content)

import os

# Define your source and target directories
source_dir = '/content/drive/MyDrive/IR_DataSet_Files/text_files-20240206T164431Z-001/text_files'
target_dir = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessed_Files'

# Get a list of filenames in the source directory (ensure consistent ordering)
source_files = sorted(os.listdir(source_dir))[:5]  # Only take the first 5 files for demonstration

for filename in source_files:
    # Define the paths for the source and target files
    source_file_path = os.path.join(source_dir, filename)
    target_file_path = os.path.join(target_dir, filename)

    # Print original content
    with open(source_file_path, 'r', encoding='utf-8') as file:
        original_content = file.read()
        print(f"Original content of {filename}:\n{original_content}\n")

    # Ensure the preprocessed file exists before attempting to open it
    if os.path.exists(target_file_path):
        # Print preprocessed content
        with open(target_file_path, 'r', encoding='utf-8') as file:
            preprocessed_content = file.read()
            print(f"Preprocessed content of {filename}:\n{preprocessed_content}\n")
    else:
        print(f"No preprocessed content found for {filename}.")

"""# **Q2. Unigram Inverted Index and Boolean Queries**"""

import pickle

# Define the path to your consolidated preprocessed text file
output_file_path = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/consolidated_preprocessed.txt'

# Initialize an empty dictionary for the inverted index
inverted_index = {}

# Open and read the preprocessed file
with open(output_file_path, 'r', encoding='utf-8') as file:
    # Assume each line corresponds to a document
    for doc_id, line in enumerate(file):
        # Tokenize the line into unigrams (since it's already preprocessed)
        unigrams = line.strip().split()
        # Update the inverted index
        for unigram in unigrams:
            if unigram in inverted_index:
                inverted_index[unigram].add(doc_id)
            else:
                inverted_index[unigram] = {doc_id}

#Serializing the Inverted Index with Pickle

# Define a path to save the serialized inverted index
index_file_path = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/inverted_index.pkl'

# Serialize and save the inverted index using pickle
with open(index_file_path, 'wb') as index_file:
    pickle.dump(inverted_index, index_file)

print(f"Inverted index created and saved to {index_file_path}")

import pickle

# Path to your serialized inverted index
index_file_path = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/inverted_index.pkl'

# Load the inverted index from the file
with open(index_file_path, 'rb') as index_file:
    inverted_index = pickle.load(index_file)


sample_terms = list(inverted_index.keys())[:10]  # Adjust the slice as needed

for term in sample_terms:
    doc_ids = inverted_index[term]
    # Filter doc_ids to only include those from the first 10 documents
    filtered_doc_ids = {doc_id for doc_id in doc_ids if doc_id < 10}
    if filtered_doc_ids:
        print(f"Term '{term}' appears in documents: {sorted(filtered_doc_ids)}")
    else:
        print(f"Term '{term}' does not appear in the first 10 documents.")

import pickle

# Specify the path to your serialized inverted index .pkl file
index_file_path = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/inverted_index.pkl'

# Use 'rb' to read in binary mode
with open(index_file_path, 'rb') as index_file:
    loaded_inverted_index = pickle.load(index_file)

print("Inverted index loaded successfully.")

# Print the first 10 keys (terms) in the inverted index
for term in list(inverted_index.keys())[:10]:
    print(term)

# # Replace 'sample_term' with the actual term you're interested in
# sample_terms = ['sample_term1', 'sample_term2']

# for term in sample_terms:
#     if term in inverted_index:
#         print(f"Documents containing '{term}': {inverted_index[term]}")
#     else:
#         print(f"'{term}' not found in the index.")

# Print the document count for the first 10 terms
#Checking the sample terms where it appears in docs
for term in list(inverted_index.keys())[:10]:
    doc_count = len(inverted_index[term])
    print(f"'{term}' appears in {doc_count} documents")

# target directory containing preprocessed files
target_dir = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessed_Files'

#  the universe set from filenames in the target directory
universe_set = {filename for filename in os.listdir(target_dir)}

print("Universe set defined successfully.")

def query_and(set1, set2):
    return set1 & set2

def query_or(set1, set2):
    return set1 | set2

def query_and_not(set1, set2):
    return set1 - set2


def query_or_not(set1, set2, universe_set):
    return set1 | (universe_set - set2)

import pickle


index_file_path = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/inverted_index.pkl'


with open(index_file_path, 'rb') as index_file:
    inverted_index = pickle.load(index_file)

# Printing the first 50 entries of the inverted index for checking how looks
for i, (term, doc_ids) in enumerate(inverted_index.items()):
    print(f"Term '{term}' appears in documents: {doc_ids}")
    if i >= 49:
        break

target_dir = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessed_Files'


preprocessed_files = sorted(os.listdir(target_dir))[:10]  #


for filename in preprocessed_files:
    file_path = os.path.join(target_dir, filename)
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
        print(f"Content of {filename}:\n{content}\n---\n")

import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('stopwords')



def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    # Tokenize the text
    tokens = word_tokenize(text)

    # Remove punctuation from each token
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]

    # Remove remaining tokens that are not alphabetic
    words = [word for word in stripped if word.isalpha()]

    # Filter out stop words
    stop_words = set(stopwords.words('english'))
    words = [w for w in words if not w in stop_words]

    # Remove blank space tokens, if any
    words = [w for w in words if w.strip()]

    return words

# Assuming the boolean operation functions and loaded_inverted_index are defined

def process_queries(queries, loaded_inverted_index, universe_set):
    for i, (terms_str, ops_str) in enumerate(queries, start=1):
        terms = terms_str.lower().split()
        ops = ops_str.split(", ")

        # Initialize the result set with the first term's document IDs
        result_set = loaded_inverted_index.get(terms[0], set())
        for op, term in zip(ops, terms[1:]):
            next_set = loaded_inverted_index.get(term, set())
            if op == "AND":
                result_set = query_and(result_set, next_set)
            elif op == "OR":
                result_set = query_or(result_set, next_set)
            elif op == "AND NOT":
                result_set = query_and_not(result_set, next_set)
            elif op == "OR NOT":
                result_set = query_or_not(result_set, next_set, universe_set)

        # Output the formatted results
        print(f"Query {i}: {' '.join(terms)} {' '.join(ops)}")
        print(f"Number of documents retrieved for query {i}: {len(result_set)}")
        doc_names = sorted([f"{doc_id}" for doc_id in result_set])  # Assuming doc_id are filenames
        print(f"Names of the documents retrieved for query {i}: {', '.join(doc_names)}\n")

# Sample queries for demonstration (replace with actual queries)
# sample_queries = [
#     (preprocess_text("Car bag in a canister"), "OR, AND NOT"),
#     (preprocess_text("Coffee brewing techniques in cookbook"), "AND, OR NOT, OR")
# ]

# Process the sample queries (assuming loaded_inverted_index and universe_set are defined)
# process_queries(sample_queries, loaded_inverted_index, universe_set)


sample_queries = [
    "Car bag in a canister",
    "Coffee brewing techniques in cookbook"
]

# # Preprocess each query
# preprocessed_queries = [preprocess_text(query) for query in sample_queries]
# #print(preprocessed_queries)

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
import pickle


nltk.download('punkt')
nltk.download('stopwords')

def preprocess_text(text):

    text = text.lower()

    tokens = word_tokenize(text)

    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]

    stop_words = set(stopwords.words('english'))
    words = [word for word in stripped if word.isalpha() and word not in stop_words]
    return words

def process_query(query_str, ops_str, loaded_inverted_index, universe_set):
    terms = preprocess_text(query_str)
    ops = ops_str.split(", ")
    result_set = set(loaded_inverted_index.get(terms[0], set()))
    query_representation = terms[0]

    for op, term in zip(ops, terms[1:]):
        next_set = set(loaded_inverted_index.get(term, set()))
        if op == "AND":
            result_set &= next_set
        elif op == "OR":
            result_set |= next_set
        elif op == "AND NOT":
            result_set -= next_set
        elif op == "OR NOT":
            result_set |= (universe_set - next_set)
        query_representation += f" {op} {term}"

    doc_names = sorted([f"{doc_id}.txt" for doc_id in result_set])
    return query_representation, len(result_set), doc_names

def user_input_and_search(loaded_inverted_index, universe_set):
    num_queries = int(input("Enter number of queries: "))
    for i in range(num_queries):
        query_str = input(f"Enter query {i+1}: ")
        ops_str = input(f"Enter operations for query {i+1} (comma-separated): ")
        query_representation, num_docs, doc_names = process_query(query_str, ops_str, loaded_inverted_index, universe_set)

        print(f"Query {i+1}: {query_representation}")
        print(f"Number of documents retrieved for query {i+1}: {num_docs}")
        if doc_names:
            print(f"Names of the documents retrieved for query {i+1}: {', '.join(doc_names)}")
        else:
            print("No documents found.")
        print()



# Calling  the user input and search function with the loaded data
user_input_and_search(loaded_inverted_index, universe_set)

# def process_queries(queries, loaded_inverted_index, universe_set):
#     results = []
#     for query_index, (query_str, ops_str) in enumerate(queries, start=1):
#         terms = preprocess_text(query_str)
#         ops = ops_str.split(", ")

#         result_set = set(loaded_inverted_index.get(terms[0], []))
#         query_representation = [terms[0]]  # Initialize with the first term

#         for term, op in zip(terms[1:], ops):
#             next_set = set(loaded_inverted_index.get(term, []))
#             if op == "AND":
#                 result_set &= next_set
#                 query_representation.append("AND " + term)
#             elif op == "OR":
#                 result_set |= next_set
#                 query_representation.append("OR " + term)
#             elif op == "AND NOT":
#                 result_set -= next_set
#                 query_representation.append("AND NOT " + term)
#             elif op == "OR NOT":
#                 result_set |= (universe_set - next_set)
#                 query_representation.append("OR NOT " + term)

#         formatted_query = ' '.join(query_representation)
#         doc_names = sorted([str(doc_id) for doc_id in result_set])
#         results.append((formatted_query, len(result_set), doc_names))
#     return results

"""# **# Q3. Positional Index and Phrase Queries**"""

source_dir = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessed_Files'


positional_index = {}

for filename in os.listdir(source_dir):
    file_path = os.path.join(source_dir, filename)
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
        words = content.split()
        for position, word in enumerate(words):
            if word not in positional_index:
                positional_index[word] = {}
            if filename not in positional_index[word]:
                positional_index[word][filename] = []
            positional_index[word][filename].append(position)

#path for saving the positional index
index_file_path = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/positional_index.pkl'

# Serializing and saving  the positional index
with open(index_file_path, 'wb') as index_file:
    pickle.dump(positional_index, index_file)

print("Positional index saved successfully.")

# Loading the Positional Index as per the document.
with open(index_file_path, 'rb') as index_file:
    loaded_positional_index = pickle.load(index_file)

print("Positional index loaded successfully.")

# Example code to inspect the positional index for specific terms
terms_to_check = ['car', 'coffee']
for term in terms_to_check:
    if term in loaded_positional_index:
        print(f"Term '{term}' is in the index.")
    else:
        print(f"Term '{term}' is not found in the index.")

def preprocess_query(query):
    # Assuming more sophisticated preprocessing if necessary
    query = query.lower()  # Lowercase the query
    tokens = query.split()  # Tokenization - assuming space-separated words
    # Include additional preprocessing steps here if they were applied in Q1
    return tokens

import pickle

# Path to your serialized positional index
index_file_path = '/content/drive/MyDrive/IR_DataSet_Files/Preprocessing_data_textfiles/positional_index.pkl'

# Load the positional index from the file
with open(index_file_path, 'rb') as index_file:
    positional_index = pickle.load(index_file)

# Print the first 50 entries of the positional index
for i, (term, doc_positions) in enumerate(positional_index.items()):
    print(f"Term '{term}' appears in documents and positions: {doc_positions}")
    if i >= 49:  # Stop after printing 50 entries
        break

import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import pickle
import nltk

nltk.download('punkt')
nltk.download('stopwords')

def preprocess_text(text):
    text = text.lower()
    tokens = word_tokenize(text)
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    words = [word for word in stripped if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    words = [w for w in words if not w in stop_words]
    return words

def find_documents_for_query(query, positional_index):
    query_terms = preprocess_text(query)
    documents = None
    for i, term in enumerate(query_terms):
        if term in positional_index:
            if documents is None:
                documents = {doc: positions for doc, positions in positional_index[term].items()}
            else:
                temp_documents = {}
                for doc, positions in documents.items():
                    if doc in positional_index[term]:
                        new_positions = [pos for pos in positional_index[term][doc] if pos-1 in positions]
                        if new_positions:
                            temp_documents[doc] = new_positions
                documents = temp_documents
        else:
            return set()
    return set(documents.keys())

def user_input_and_search(loaded_positional_index):
    n = int(input("Enter number of queries: "))
    for i in range(n):
        query = input(f"Enter query {i+1}: ")
        matching_docs = find_documents_for_query(query, loaded_positional_index)
        print(f"Number of documents retrieved for query {i+1} using positional index: {len(matching_docs)}")
        if matching_docs:
            print(f"Names of documents retrieved for query {i+1} using positional index: {', '.join(sorted(matching_docs))}")
        else:
            print("No documents found.")



user_input_and_search(loaded_positional_index)

